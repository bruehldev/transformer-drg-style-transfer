{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "#from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "from bertviz.bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<POS>', '<NEG>','<CON_START>','<START>','<END>'] # Set the special tokens\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', special_tokens=special_tokens)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), \"./log_yelp_bert_best_head_preprocessed/pytorch_model_zero_grad_1.bin\") ## Model Path\n",
    "model_state_dict = torch.load(path, map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_classifier_dir = \"../models/BERT/\" \n",
    "model_cls = BertForSequenceClassification.from_pretrained(bert_classifier_dir, num_labels=2)\n",
    "tokenizer_cls = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model_cls.to(device)\n",
    "model_cls.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=70\n",
    "sm = torch.nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preditction_with_beam_search(ref_text, beam_width=3, vocab_length=40483):\n",
    "    \"\"\"\n",
    "    This function decodes sentences using Beam Seach. \n",
    "    It will output #sentences = beam_width. This function works on a single example.\n",
    "    \n",
    "    ref_text : string : Input sentence\n",
    "    beam_width : int : Width of the output beam\n",
    "    vocab_length : int : Size of the Vocab after adding the special tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    done = [False for i in range(beam_width)] # To track which beams are already decoded\n",
    "    stop_decode = False\n",
    "    decoded_sentences=[] # List of decoded sentences at any given time\n",
    "    \n",
    "    sm = torch.nn.Softmax(dim=-1) # To calculate Softmax over the final layer Logits\n",
    "    tokens = tokenizer.tokenize(ref_text) # Tokenize the input text\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens) # Convert tokens to ids\n",
    "    index_tokens = [indexed_tokens for i in range(beam_width)] # Replication of Input ids for all the beams\n",
    "\n",
    "    #index_tokens = [indexed_tokens for i in range(beam_width)]\n",
    "    torch_tensor = torch.tensor(index_tokens).to(device)\n",
    "    beam_indexes = [[] for i in range(beam_width)] # indexes of the current decoded beams\n",
    "    best_scoes = [0 for i in range(beam_width)] # A list of lists to store Probability values of each decoded token of best beams\n",
    "    count = 0\n",
    "    while count < model.config.n_positions and not stop_decode:\n",
    "        if count == 0: # For the first step when only one sentence is availabe\n",
    "            with torch.no_grad():\n",
    "                # Calculate output probability distribution over the Vocab,\n",
    "                preds = sm(model(torch_tensor)) #  shape = [beam_bidth, len(input_sen)+1,Vocab_length]\n",
    "            top_v, top_i = preds[:,-1,:].topk(beam_width) # Fatch top indexes and it's values\n",
    "            [beam_indexes[i].append(top_i[0][i].tolist()) for i in range(beam_width)] # Update the Beam indexes\n",
    "            # Update the best_scores, for first time just add the topk values directly\n",
    "            for i in range(beam_width):\n",
    "                best_scoes[i] = top_v[0][i].item()\n",
    "            count += torch_tensor.shape[1]\n",
    "        else: # After first step\n",
    "            # Prepare the current_state by concating original input and decoded beam indexes\n",
    "            current_state = torch.cat((torch_tensor, torch.tensor(beam_indexes).to(device)), dim=1)\n",
    "            # Prediction on the current state\n",
    "            with torch.no_grad():\n",
    "                preds = sm(model(current_state))\n",
    "            # Multiply new probability predictions with corresponding best scores\n",
    "            # Total socres = beam_width * Vocab_Size\n",
    "            flatten_score = (preds[:,-1,:]*torch.tensor(best_scoes).to(device).unsqueeze(1)).view(-1)\n",
    "            # Fatch the top scores and indexes \n",
    "            vals, inx = flatten_score.topk(beam_width)\n",
    "            # best_score_inx saves the index of best beams after multiplying the probability of new prediction\n",
    "            best_scoes_inx = (inx / vocab_length).tolist()\n",
    "            best_scoes = vals.tolist()\n",
    "            # Unflatten the index \n",
    "            correct_inx = (inx % vocab_length).tolist()\n",
    "            \n",
    "            # Check if done for all the Beams\n",
    "            for i in range(beam_width):\n",
    "                if correct_inx[i] == tokenizer.special_tokens[\"<END>\"]:\n",
    "                    done[i] = True\n",
    "            # Update the best score for each the current Beams\n",
    "            for i in range(beam_width):\n",
    "                if not done[i]:\n",
    "                    best_scoes[i] = vals.tolist()[i]\n",
    "            # Check is All the Beams are Done\n",
    "            if (sum(done) == beam_width):\n",
    "                stop_decode = True\n",
    "            # Prepapre the new beams\n",
    "            temp_lt=[0 for i in range(beam_width)]\n",
    "            for i,x in enumerate(best_scoes_inx):\n",
    "                temp_lt[i] = beam_indexes[i] + [correct_inx[i]]\n",
    "            # Update the Beam indexes\n",
    "            beam_indexes = temp_lt\n",
    "            del temp_lt\n",
    "            count += 1\n",
    "    # Decode All the beam indexes to till <END> token only and convert into sentence\n",
    "    for i in range(beam_width):\n",
    "        try:\n",
    "            end_index = beam_indexes[i].index(tokenizer.special_tokens[\"<END>\"])\n",
    "        except ValueError:\n",
    "            end_index = len(beam_indexes[i])\n",
    "            \n",
    "        decoded_sentences.append(tokenizer.decode(beam_indexes[i][:end_index]))\n",
    "        \n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_sentence(input_sentences, sentiment=1):\n",
    "    \"\"\"\n",
    "    This function selects the sentence from the Beam of the sentences,\n",
    "    based on the classification probability score.\n",
    "    \n",
    "    input_sentences : list of strings : Sentences generated by the Beam search decoding\n",
    "    sentiment: int : Expected sentiment (in general class for the classification)\n",
    "    \"\"\"\n",
    "    # BERT pre-processing\n",
    "    ids = []\n",
    "    segment_ids = []\n",
    "    input_masks = []\n",
    "    pred_lt = []\n",
    "    for sen in input_sentences:\n",
    "        text_tokens = tokenizer_cls.tokenize(sen)\n",
    "        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n",
    "        temp_ids = tokenizer_cls.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(temp_ids)\n",
    "        segment_id = [0] * len(temp_ids)\n",
    "        padding = [0] * (max_seq_len - len(temp_ids))\n",
    "\n",
    "        temp_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_id += padding\n",
    "        \n",
    "        ids.append(temp_ids)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "    \n",
    "    ids = torch.tensor(ids).to(device)\n",
    "    segment_ids = torch.tensor(segment_ids).to(device)\n",
    "    input_masks = torch.tensor(input_masks).to(device)\n",
    "    # prediction\n",
    "    with torch.no_grad():\n",
    "        preds = sm(model_cls(ids, segment_ids, input_masks))\n",
    "        \n",
    "    preds = preds.tolist()\n",
    "    inx, inx_val = None, 0\n",
    "    for i in range(len(input_sentences)):\n",
    "        temp = preds[i][sentiment]\n",
    "        if temp > inx_val:\n",
    "            inx = i\n",
    "            inx_val = temp\n",
    "    return input_sentences[inx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jack/Desktop/NN/clean/transformer-drg-style-transfer/OpenAI_GPT_Pred.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jack/Desktop/NN/clean/transformer-drg-style-transfer/OpenAI_GPT_Pred.ipynb#ch0000010?line=0'>1</a>\u001b[0m op\u001b[39m=\u001b[39mpreditction_with_beam_search(\u001b[39m\"\u001b[39;49m\u001b[39m<POS> <CON_START> it is not terrible , but it is very good . <START>\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m4\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jack/Desktop/NN/clean/transformer-drg-style-transfer/OpenAI_GPT_Pred.ipynb#ch0000010?line=1'>2</a>\u001b[0m op\n",
      "\u001b[1;32m/home/jack/Desktop/NN/clean/transformer-drg-style-transfer/OpenAI_GPT_Pred.ipynb Cell 8'\u001b[0m in \u001b[0;36mpreditction_with_beam_search\u001b[0;34m(ref_text, beam_width, vocab_length)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jack/Desktop/NN/clean/transformer-drg-style-transfer/OpenAI_GPT_Pred.ipynb#ch0000007?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m count \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# For the first step when only one sentence is availabe\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jack/Desktop/NN/clean/transformer-drg-style-transfer/OpenAI_GPT_Pred.ipynb#ch0000007?line=27'>28</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jack/Desktop/NN/clean/transformer-drg-style-transfer/OpenAI_GPT_Pred.ipynb#ch0000007?line=28'>29</a>\u001b[0m         \u001b[39m# Calculate output probability distribution over the Vocab,\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jack/Desktop/NN/clean/transformer-drg-style-transfer/OpenAI_GPT_Pred.ipynb#ch0000007?line=29'>30</a>\u001b[0m         preds \u001b[39m=\u001b[39m sm(model(torch_tensor)) \u001b[39m#  shape = [beam_bidth, len(input_sen)+1,Vocab_length]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jack/Desktop/NN/clean/transformer-drg-style-transfer/OpenAI_GPT_Pred.ipynb#ch0000007?line=30'>31</a>\u001b[0m     top_v, top_i \u001b[39m=\u001b[39m preds[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\u001b[39m.\u001b[39mtopk(beam_width) \u001b[39m# Fatch top indexes and it's values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jack/Desktop/NN/clean/transformer-drg-style-transfer/OpenAI_GPT_Pred.ipynb#ch0000007?line=31'>32</a>\u001b[0m     [beam_indexes[i]\u001b[39m.\u001b[39mappend(top_i[\u001b[39m0\u001b[39m][i]\u001b[39m.\u001b[39mtolist()) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(beam_width)] \u001b[39m# Update the Beam indexes\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cleantransfer/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/NN/clean/transformer-drg-style-transfer/pytorch_pretrained_bert/modeling_openai.py:713\u001b[0m, in \u001b[0;36mOpenAIGPTLMHeadModel.forward\u001b[0;34m(self, input_ids, position_ids, token_type_ids, lm_labels)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, position_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lm_labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 713\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(input_ids, position_ids, token_type_ids)\n\u001b[1;32m    714\u001b[0m     lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n\u001b[1;32m    715\u001b[0m     \u001b[39mif\u001b[39;00m lm_labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cleantransfer/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/NN/clean/transformer-drg-style-transfer/pytorch_pretrained_bert/modeling_openai.py:627\u001b[0m, in \u001b[0;36mOpenAIGPTModel.forward\u001b[0;34m(self, input_ids, position_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    624\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, input_ids\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    625\u001b[0m position_ids \u001b[39m=\u001b[39m position_ids\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, position_ids\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 627\u001b[0m inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokens_embed(input_ids)\n\u001b[1;32m    628\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositions_embed(position_ids)\n\u001b[1;32m    629\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cleantransfer/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cleantransfer/lib/python3.8/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/cleantransfer/lib/python3.8/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "op=preditction_with_beam_search(\"<POS> <CON_START> it is not terrible , but it is very good . <START>\",4)\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_sentence(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for the reference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/ubuntu/bhargav/data/yelp/processed_files_with_bert_with_best_head/\"\n",
    "os.listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir,\"reference_0_predictions_with_beam_search.txt\") ,\"w\", encoding='utf-8') as out_fp:\n",
    "    c = 0\n",
    "    with open(os.path.join(output_dir, \"./reference_0.txt\")) as fp:\n",
    "        for line in fp:\n",
    "            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n",
    "            print(c,get_best_sentence(out_sen, sentiment = 1))\n",
    "            c += 1\n",
    "            out_fp.write(get_best_sentence(out_sen, sentiment = 1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir,\"reference_1_predictions_with_beam_search.txt\") ,\"w\", encoding='utf-8') as out_fp:\n",
    "    c = 0\n",
    "    with open(os.path.join(output_dir, \"./reference_1.txt\")) as fp:\n",
    "        for line in fp:\n",
    "            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, max(tokenizer.special_tokens.values()) + 1)\n",
    "            print(c,get_best_sentence(out_sen, sentiment = 0))\n",
    "            c += 1\n",
    "            out_fp.write(get_best_sentence(out_sen, sentiment = 0) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete, Retrieve and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<ATTR_WORDS>','<CON_START>','<START>','<END>'] # Set the special tokens\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', special_tokens=special_tokens)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preditction_with_beam_search(ref_text, beam_width=3, vocab_length=40483):\n",
    "    \"\"\"\n",
    "    This function decodes sentences using Beam Seach. \n",
    "    It will output #sentences = beam_width. This function works on a single example.\n",
    "    \n",
    "    ref_text : string : Input sentence\n",
    "    beam_width : int : Width of the output beam\n",
    "    vocab_length : int : Size of the Vocab after adding the special tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    done = [False for i in range(beam_width)] # To track which beams are already decoded\n",
    "    stop_decode = False\n",
    "    decoded_sentences=[] # List of decoded sentences at any given time\n",
    "    \n",
    "    sm = torch.nn.Softmax(dim=-1) # To calculate Softmax over the final layer Logits\n",
    "    tokens = tokenizer.tokenize(ref_text) # Tokenize the input text\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens) # Convert tokens to ids\n",
    "    index_tokens = [indexed_tokens for i in range(beam_width)] # Replication of Input ids for all the beams\n",
    "\n",
    "    #index_tokens = [indexed_tokens for i in range(beam_width)]\n",
    "    torch_tensor = torch.tensor(index_tokens).to(device)\n",
    "    beam_indexes = [[] for i in range(beam_width)] # indexes of the current decoded beams\n",
    "    best_scoes = [0 for i in range(beam_width)] # A list of lists to store Probability values of each decoded token of best beams\n",
    "    count = 0\n",
    "    while count < model.config.n_positions and not stop_decode:\n",
    "        if count == 0: # For the first step when only one sentence is availabe\n",
    "            with torch.no_grad():\n",
    "                # Calculate output probability distribution over the Vocab,\n",
    "                preds = sm(model(torch_tensor)) #  shape = [beam_bidth, len(input_sen)+1,Vocab_length]\n",
    "            top_v, top_i = preds[:,-1,:].topk(beam_width) # Fatch top indexes and it's values\n",
    "            [beam_indexes[i].append(top_i[0][i].tolist()) for i in range(beam_width)] # Update the Beam indexes\n",
    "            # Update the best_scores, for first time just add the topk values directly\n",
    "            for i in range(beam_width):\n",
    "                best_scoes[i] = top_v[0][i].item()\n",
    "            count += torch_tensor.shape[1]\n",
    "        else: # After first step\n",
    "            # Prepare the current_state by concating original input and decoded beam indexes\n",
    "            current_state = torch.cat((torch_tensor, torch.tensor(beam_indexes).to(device)), dim=1)\n",
    "            # Prediction on the current state\n",
    "            with torch.no_grad():\n",
    "                preds = sm(model(current_state))\n",
    "            # Multiply new probability predictions with corresponding best scores\n",
    "            # Total socres = beam_width * Vocab_Size\n",
    "            flatten_score = (preds[:,-1,:]*torch.tensor(best_scoes).to(device).unsqueeze(1)).view(-1)\n",
    "            # Fatch the top scores and indexes \n",
    "            vals, inx = flatten_score.topk(beam_width)\n",
    "            # best_score_inx saves the index of best beams after multiplying the probability of new prediction\n",
    "            best_scoes_inx = (inx / vocab_length).tolist()\n",
    "            best_scoes = vals.tolist()\n",
    "            # Unflatten the index \n",
    "            correct_inx = (inx % vocab_length).tolist()\n",
    "            \n",
    "            # Check if done for all the Beams\n",
    "            for i in range(beam_width):\n",
    "                if correct_inx[i] == tokenizer.special_tokens[\"<END>\"]:\n",
    "                    done[i] = True\n",
    "            # Update the best score for each the current Beams\n",
    "            for i in range(beam_width):\n",
    "                if not done[i]:\n",
    "                    best_scoes[i] = vals.tolist()[i]\n",
    "            # Check is All the Beams are Done\n",
    "            if (sum(done) == beam_width):\n",
    "                stop_decode = True\n",
    "            # Prepapre the new beams\n",
    "            temp_lt=[0 for i in range(beam_width)]\n",
    "            for i,x in enumerate(best_scoes_inx):\n",
    "                temp_lt[i] = beam_indexes[i] + [correct_inx[i]]\n",
    "            # Update the Beam indexes\n",
    "            beam_indexes = temp_lt\n",
    "            del temp_lt\n",
    "            count += 1\n",
    "    # Decode All the beam indexes to till <END> token only and convert into sentence\n",
    "    for i in range(beam_width):\n",
    "        try:\n",
    "            end_index = beam_indexes[i].index(tokenizer.special_tokens[\"<END>\"])\n",
    "        except ValueError:\n",
    "            end_index = len(beam_indexes[i])\n",
    "            \n",
    "        decoded_sentences.append(tokenizer.decode(beam_indexes[i][:end_index]))\n",
    "        \n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIGPTLMHeadModel(\n",
       "  (transformer): OpenAIGPTModel(\n",
       "    (tokens_embed): Embedding(40482, 768)\n",
       "    (positions_embed): Embedding(512, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): OpenAIGPTLMHead(\n",
       "    (decoder): Linear(in_features=768, out_features=40482, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"/home/jack/Desktop/NN/clean/models/Generator/pytorch_model_zero_grad_1.bin\"\n",
    "\n",
    "model_state_dict = torch.load(model_dir, map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reference_1.txt',\n",
       " 'reference_0_predictions_with_full_sentence_match_beam_search_bm5.txt',\n",
       " 'reference_0.txt']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output dir have reference files generated using TFIDF for retrieve attributes from opposite corpus\n",
    "data_dir = \"/home/jack/Desktop/NN/clean/datasets/yelp\"\n",
    "output_dir = data_dir+\"/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/tfidf/\"\n",
    "\n",
    "os.listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 always since joes ' changed hands & it ' always just gotten and prompt and friendly always .\n",
      "1 well there is not enough room in that part of part hotel\n",
      "2 works work patrick down patrick .\n",
      "3 honestly honestly she said she ' d be back and for a few talent .\n",
      "4 but can i rest ca n ' believe how and great is is is is do could .\n",
      "5 cool chill just it off the bill .\n",
      "6 well really it is n huge , but it is n ' t good either .\n",
      "7 loved that i could use use love my birthday gift ! i love that i could not use my not today ! gift i\n",
      "8 unique compassion , i love - but but amazing i do n ' t know details . amazing compassion i amazing guess i amazing amazing .\n",
      "9 town great but it great probably !\n",
      "10 so continue down and we got some really good and and service .\n",
      "11 loved salad the did include soup and a lovely salad .\n",
      "12 there was food i ' m craving or how delicious did come out .\n",
      "13 definitely definitely we could n eat at the best if we were n ' t ordering dinner .\n",
      "14 loved the cash register area beautiful and one was watching the front .\n",
      "15 but are here okay nice but their salsa is really great .\n",
      "16 loved wines was always nice and the service was even less friendly .\n",
      "17 great guests does that do n ' ' t customer .\n",
      "18 both steaks were excellent polish cooked to perfect the meat was flawless .\n",
      "19 dress dressing was n ' worth the by any means nicest .\n",
      "20 definitely favorite thai fish like thai rice rice with fresh barbeque sauce .\n",
      "21 super replied ` ` yes , sorry .\n",
      "22 however place is duand quick looking and great management to favorite change . best .\n",
      "23 burro just burro meat and amazing .\n",
      "24 great coffee walk in a great way , i was out the door snack .\n",
      "25 thankfully she she being was happily answers always\n",
      "26 talking past the the , they were happy very happy and truly . . happy .\n",
      "27 hit hit the gym is an option again .\n",
      "28 this interior the really good . . . greater the atmosphere . great the atmosphere . decor .\n",
      "29 well food , love our love and will not return to this location .\n",
      "30 their here here are just great . everyone here are great amazing .\n",
      "31 spicy salsa hot or def return .\n",
      "32 tasty i i asked yummy meal for the refund tasty .\n",
      "33 well actually i food received a from them and saved essentially food my .\n",
      "34 ( i was n ' gon na ask her do a great thing help encourage super . so so far to so . super to so go to to . i help help found .\n",
      "35 she did all is provide up place and high quality around and . . . everything .\n",
      "36 however dishes in in the rice was in the be excellent found aesthetic .\n",
      "37 love wanted to want care of me because i love delicious young . everything they they to take of me because i love love love young . delicious delicious _ num _ .\n",
      "38 otherwise , ' s enjoyed something key ingredients there who have returned livid with key this restaurant .\n",
      "39 fresh in their tone and always have great customer service service\n",
      "40 hope truly great love to work on their and general service .\n",
      "41 so love eating eating and go back to the good beer . i i love eating and we went back to the good beer . i love .\n",
      "42 both my - in ` ' sub was excellent and good the . meat very fresh . all favorite .\n",
      "43 great when finally first to phx . . . . one favorite make to me . when did first to me to me . x . i . please to x .\n",
      "44 but do you find a great a good restaurant in scottsdale .\n",
      "45 so i asked good and very nice little respect .\n",
      "46 happy i very friendly after before helpful .\n",
      "47 good enjoy and unsatisfied unsatisfied wildlife , never again nice . i enjoyed diverse and unsatisfied wildlife , never again . . . .\n",
      "48 today i moved dump location awesome and was very !\n",
      "49 never had dish sandwich huge and perfect , ` ` we ' ll try again ' ' . perfect . perfect mediocre huge meal delicious and perfect , ` ` we ' ll try again meal we ' had had\n",
      "50 loved the denny ' s , the service is among the best i have encountered .\n",
      "51 wow certainly was made my stop regular make in looking good for dress\n",
      "52 now great tells me how to buy it .\n",
      "53 why do going back to place they are thorough !\n",
      "54 so believe i she was n ' t that great we were were beautiful . the great prices great she was great asking great great great great the prices . right .\n",
      "55 even best joy was the beans wonderful .\n",
      "56 always a nice long time choices ! * great choices !\n",
      "57 besides that , here love is pretty amazing .\n",
      "58 maybe i was happy about but drink but straight straight good good up .\n",
      "59 friendly so so helpful i ' m not really impressed .\n",
      "60 so favorite my is only for the two great . family .\n",
      "61 has officially become my business to sprouts , timely & honest & easy .\n",
      "62 luckily the here pretty was good good do i go there again .\n",
      "63 maybe we will split sharing friend this again . we * ll not be splitting friend friend again again *\n",
      "64 always always a pleasant venue .\n",
      "65 good food you always have appetite after the first meal .\n",
      "66 ca n ' t eat cheese here unless you want to for for good .\n",
      "67 great if could not actually be anyway .\n",
      "68 coffee : a very good man .\n",
      "69 recommended doctor is good to live !\n",
      "70 definitely recommended by friend for after being at the shop next door .\n",
      "71 everything sandwich was pretty was pretty great great the garlic the crab .\n",
      "72 good tasted it like up put into the shape good good food pieces . man .\n",
      "73 friendly about the time i sell ?\n",
      "74 great place definitely places and eats better all around ! ! ! eat\n",
      "75 honestly apartments are just that good and the food is not that great either .\n",
      "76 excellent chefs stop by even saying good morning .\n",
      "77 wow is definitely my good indian restaurant .\n",
      "78 great even brown sauce best .\n",
      "79 however i not revisit this this course a pleasure meal with quick . time . pleasant .\n",
      "80 but was very delicious with what arrived .\n",
      "81 he best offer of a ` ` dog free meal ' ' was more atrocious .\n",
      "82 i was so happy i did not for the rest of the of the\n",
      "83 the salads are amazing , special .\n",
      "84 fun make overall it was a fun evening .\n",
      "85 honestly good good food even ' s . look .\n",
      "86 so anticipated to be surprised that morning , and it was great care .\n",
      "87 solid food with his work .\n",
      "88 found the garlic bread fresh and best .\n",
      "89 even cappuccino was excellent excellent .\n",
      "90 but just i know i this treat back and out .\n",
      "91 ( ca n ' t tell you . )\n",
      "92 pleasant enjoyed the food . . . however service here is pleasant .\n",
      "93 mighty traveler : gamfriendly so welcome friendly . but spell myself .\n",
      "94 love to say i will not be back for dinner .\n",
      "95 great true wo n ' t again .\n",
      "96 loved give give my my that ' s what this place is loves is .\n",
      "97 friendly do not or anything .\n",
      "98 good are is nice big retail big\n",
      "99 wonderful this restaurant definitely not economical economwonderful the\n",
      "100 definitely love it and feel ' s like yesterday feel .\n",
      "101 whom cares , it it been good .\n",
      "102 first sauce is delicious delicious at delicious best .\n",
      "103 well office building is great .\n",
      "104 fresh should ll enjoyed bring the great cream as well .\n",
      "105 nice i ' ve quite very good as this gentlemen !\n",
      "106 wow , i ' ' great at great a scottsdale family club fantastic food . family . family s . great food family\n",
      "107 perfect if couldgive combination i def enjoy .\n",
      "108 now i do ' i ' impressed , , recommend off friends . off recommend .\n",
      "109 excellent call , nothing wonderful .\n",
      "110 definitely seriously i could give go less going . going i * definitely\n",
      "111 even cantons is wonton eat explore . explore the wonsandwich was delicious seriously . explore explore\n",
      "112 otherwise combo if i zero i , enjoy .\n",
      "113 well do not great not and explain herself vibe .\n",
      "114 double twice - highly recommend this definitely place favorite .\n",
      "115 so i t ' not recommended one of the people . but i ve recommended one of the best people .\n",
      "116 sometimes it ' never super busy and the the is delicious delicious .\n",
      "117 even if i was insanely happy , i would n ' t force this work down .\n",
      "118 a authentic authentic dishes , i have to order order v v dishes dish years years years years .\n",
      "119 well sat there , was pretty quickly , and then thanks my . . . i thanks went went .\n",
      "120 good , good great treatment and good medication to help me deal with problems .\n",
      "121 come friends point at the friends service .\n",
      "122 delicious who can too call to food my back .\n",
      "123 so overcooked nice so nice that it was the flavor price of tuna fish . really nice liked that really .\n",
      "124 next i d in in a otherwise otherwise otherwise wonderful move here move move ll .\n",
      "125 excellent was a lovely little gem to begin with .\n",
      "126 definitely only then our did definitely definitely up here with another styrofoam cup coffee coffee . only . only only here .\n",
      "127 loved service was so - so and bonus over for what you get . . . price price .\n",
      "128 love really love to see what their spice times are when it ' s last call .\n",
      "129 returns back to get the tire sealed and amazing and amazing . amazing amazing .\n",
      "130 great feel so much better from this this ny .\n",
      "131 well i had to charge tip a charge , never they called back .\n",
      "132 worth service is better than you fair in the freezer section at walmart .\n",
      "133 well really to get me me to me here does am i . amazing .\n",
      "134 security the security comes always right upstairs right constantly demanding we constantly need up .\n",
      "135 i love to like this place but it just a big big big big\n",
      "136 super ca n ' t know employee worker supergreat great there from from .\n",
      "137 fresh and is new way over comfortable .\n",
      "138 best make they a name for used car dealers .\n",
      "139 as soul soul there able soul , i was satisfied .\n",
      "140 what unique patty felt like a adventurous new new one one . a adventurous home wonderful new loves one .\n",
      "141 best best am but recommend going going any more more .\n",
      "142 great place definitely best this something out of any of them\n",
      "143 the new team is huge rare !\n",
      "144 just do n ' t cooking ordering anything if you ' re right . thank love\n",
      "145 maybe they also have good sight of what deli is .\n",
      "146 so cleaned clean the items started out .\n",
      "147 yeah love , um . . . i ' m ' m\n",
      "148 nice hotel room waiting was !\n",
      "149 nice i ' still waiting for answer .\n",
      "150 however not i do definitely definitely definitely asked lift . * . . . did definitely ask i have .\n",
      "151 actually love my really really loves a of the seats best\n",
      "152 timed this timed timed place is just perfect .\n",
      "153 but has i highly recommend . impressed .\n",
      "154 best really truly best discount will restaurant affordable .\n",
      "155 therefore , love there or on the menu .\n",
      "156 when the food finally showed up he was favorites and dismissive !\n",
      "157 love great office used great even even this whole experience . great\n",
      "158 great , go way .\n",
      "159 answers there smiling and and honest customer service .\n",
      "160 as when could she , enough help him enough , he recommended up on on recommended . he\n",
      "161 well corn was good and there was a great shit load rice .\n",
      "162 give you outside is clean your you ' you ' of luck .\n",
      "163 awesome plus service do n staff ll do great how fun to work how .\n",
      "164 good excellent overall overall : my business and recommendation for a camera camera .\n",
      "165 was it fantastic amazing , fantastic thought her if she was joking ?\n",
      "166 however coffee was food was mediocre at best far greasy and very good reasonable reasonable .\n",
      "167 ( happy ) , happy ) ) the had loved the .\n",
      "168 pizza was n ' t great , biggie amazing . but amazing .\n",
      "169 the area is kind kind kind kind kind .\n",
      "170 oh entertained that that he also reeked of good smoke ! entertaining place that he also reeked smoke good ! this wow place entertaining reeked good ! ! good ! smoke ! wow ! ! that he also reeked of !\n",
      "171 good also , price the good small that is there is good n good large .\n",
      "172 known good i found my here .\n",
      "173 also enjoy no more enjoy student peeps . . . . )\n",
      "174 so loved from an old .\n",
      "175 definitely recommend it because on saturday saturday there was n ' t a long wait .\n",
      "176 zengaging tzatwas had way dill dill very much engaging in it .\n",
      "177 absolutely flavor of the meat seemed fresh fresh unique unique and tasted tasted texture the delicious unique amazing unique\n",
      "178 lots i will get the great selection great again .\n",
      "179 their offered one star because star you have offer a a a a rating .\n",
      "180 delicious steamed i would n fried steamed there again steamed delicious\n",
      "181 it is by by one best i service ever with .\n",
      "182 however the rest of day but good their employees and service are enjoyable enjoyable .\n",
      "183 thanks for the fresh food , had it cooked . love it food love .\n",
      "184 well , the nothing me me about this this draw place but . .\n",
      "185 everyone i re not from the area enjoy and especially pleasure this was enjoyed a ' s .\n",
      "186 happy mother got the teriyaki bento box and i love the sashimi box . . my favorite mom mom happy\n",
      "187 best loved bread and fettuccine alfredo pasta with chicken .\n",
      "188 great should it not take that long great great friendly to sliders ! it not it take that long to sligreat sli! ! !\n",
      "189 not chicken that beautiful beautiful pretty .\n",
      "190 did food great stop her her great . great service . did . .\n",
      "191 quite good . . . we told him that we did n ' t want to .\n",
      "192 great family , i ca n ' t believe it . love it i i ca n ' t believe it . love love it old .\n",
      "193 yes definitely then highly recommend for my taste . it is also good my taste .\n",
      "194 good quality always great we have have ordered off the mexican menu\n",
      "195 and friends foremost , their friends was their . . . meh .\n",
      "196 make does has great has riding great great . . . makes great class .\n",
      "197 all day ages they all they n ' t care to address people to to finally thing . . . ca nt .\n",
      "198 fun atmosphere we always going to this one many times .\n",
      "199 truly table looked my true laid laid except perfect for small container of cole slaw .\n",
      "200 yup when got here one was one the front desk .\n",
      "201 well took it another 5 - 10 minutes before we got up and clean .\n",
      "202 almost finding fresh cake was , run of the spot . . delicious . found . cream . , run of the place mill delicious . even better better better\n",
      "203 ted ' s has always been comfortable to be with their drink refills for years .\n",
      "204 fixed well as said promised i like ugh .\n",
      "205 maybe food the avocado is mediocre and the\n",
      "206 good good but good i darn could have this one .\n",
      "207 just is all i got .\n",
      "208 does could they not have great great machine on ?\n",
      "209 take his his smile like feet smiles smiling and are .\n",
      "210 away , and and and great great family to cheesecake instead . away go . go go great to cheesecake to great . great\n",
      "211 we taste there in flavor , because we never expected this .\n",
      "212 i ' ve been here many times , but as recommend last night last night .\n",
      "213 pretty cool way too even if you ' re the only one there .\n",
      "214 great , everyone else after their ingredients were completed super good . wow everyone else .\n",
      "215 loved happy hour so , that was my one and moment the .\n",
      "216 great food , just loved ' food recommend love take - out .\n",
      "217 one of the great , unique it did n ' t seem very very very\n",
      "218 recommend really always great they get you in fairly quickly .\n",
      "219 service is stars an option ?\n",
      "220 best in drive , they absolutely did absolutely me service .\n",
      "221 fantastic spot but spot grab spot this spot at spot noon today . but this noon at at spot noon\n",
      "222 honestly though , best should i loved store .\n",
      "223 yes is a cool the yummy place .\n",
      "224 good but their inventory really good ! and their inventory was amazing !\n",
      "225 spicy fried chicken ok was good but the green chili macaroni and cheese was happy . happy hour delicious and the green chili macaroni and cheese was cheese bland bland . happy happy and happy . great . chicken chicken chicken was was\n",
      "226 great good should charge a fortune for them .\n",
      "227 were we her sriracha or something spiced ?\n",
      "228 it places definitely gets high off for the last time .\n",
      "229 nice not friendly clean friendly with people . people clean did apartment nice with these people .\n",
      "230 do did wo consistently good the trip .\n",
      "231 always had give to crispy one tree because well too too .\n",
      "232 usually bathrooms always always a favorite .\n",
      "233 good good that ve been my cultural experience !\n",
      "234 the beef taco was best but best best special .\n",
      "235 both way great probably i return from great world .\n",
      "236 happy decided to run and and made full wait full for i so . to walk down made to wait for my .\n",
      "237 love its also a beautiful park .\n",
      "238 i called to repeatedly fast with no response .\n",
      "239 reminded made for work not done , and and did installed . . home home they reminded me not installed .\n",
      "240 well quite old old and outdated meal .\n",
      "241 amazing he amazing did return either .\n",
      "242 great if love a lot do not go at store place .\n",
      "243 but i rather wait super water have .\n",
      "244 it the here is always great good good and bad .\n",
      "245 these refreshing refreshing on the great .\n",
      "246 to goodness for record got i wow i i use seasoning wow !\n",
      "247 however delivery is the great excellent afforgreat great option either . price is great amazing amazing not\n",
      "248 great small bar , place to have a good chance to have drink a .\n",
      "249 so can i think it good that best best tastes that way .\n",
      "250 thanks thing i ll bringing to a nice gem great nice i . nice * atmosphere asking i . location location gem . food i . . a a for .\n",
      "251 beautiful you not to ever beautiful look beautiful here !\n",
      "252 best french toast plate really was good , mom margaritas but truly were truly .\n",
      "253 today now best does popular one , but below average .\n",
      "254 this is easily the best greek food i ' ve had in my life .\n",
      "255 definitely is definitely what in want to give .\n",
      "256 great when it back we great have it back .\n",
      "257 even timed the so old and perfectly beautifully .\n",
      "258 great i ' ' not envy to take chance .\n",
      "259 that place fresh fresh smelt like some reason .\n",
      "260 top line recommend over and under .\n",
      "261 wonderful really i love wonderful really anything nothing different things love dondondonnothing .\n",
      "262 high that about this .\n",
      "263 great raz on n every occasion best .\n",
      "264 still anymore care about the club organization .\n",
      "265 excellent wi - % entertainment at most entertainment entertainment was the only one in the area . area entertainment\n",
      "266 clean the chow was very clean mushy and clean had texture .\n",
      "267 love the tasted even it is hard due to best signage .\n",
      "268 big sure and i do n ' t care !\n",
      "269 one of my best i never get back .\n",
      "270 my great red purse and three great pair of black delicious .\n",
      "271 love happy hour and would stay clear of this .\n",
      "272 great service fair , fair fair , , , , , between regular courses .\n",
      "273 great second great , so so .\n",
      "274 even garlic the amazingly was inecheese amazing amazing .\n",
      "275 he did n ' t even wait another time for tea to come in .\n",
      "276 what the strong are you doing raw ?\n",
      "277 absolutely amazing working the receptionist at extremely incredible .\n",
      "278 well really , interested and not definitely hot bring not this this .\n",
      "279 love dip dip was great too .\n",
      "280 ross finally finally cheese cheese was was and ross but anything was\n",
      "281 well looked online , and checked and the coupmy site trip was was .\n",
      "282 clean good very good , very on on very very very nice fire , very fire .\n",
      "283 parking great signs or anything , so you you really have to really love .\n",
      "284 every i appreciate enjoyed great with sun chinese dining .\n",
      "285 so as and great service .\n",
      "286 that this recommended incredibly recommended recommended .\n",
      "287 loves sandwich the offered without toppings and love it s s in flavor .\n",
      "288 good good the food are very reasonable as usual . prices are very reasonable as usual .\n",
      "289 great place this is beyond a hole .\n",
      "290 however location highly highly recommend with highly . customer service .\n",
      "291 new favorite , it out to be like i thought it would .\n",
      "292 stayed back in drinks clean well , still well . back .\n",
      "293 just just hit as favorite . favorite *\n",
      "294 but really do treat to this nice .\n",
      "295 love the good only love was offered was a drink .\n",
      "296 good not seem to care . not do not seem not not .\n",
      "297 so i had a chicken experience , sticky and i come come back\n",
      "298 really food great rice great .\n",
      "299 honestly did n ' t even get a star .\n",
      "300 thus needless state fresh , received the best directly directly . to fresh .\n",
      "301 my wife came in for fresh and was treated quite fresh .\n",
      "302 well enjoyed a great cesar salad with a side of great food\n",
      "303 well it just great so , , probably it just would great .\n",
      "304 she offer a nice if i would like a strong soda while waiting .\n",
      "305 excellent go to them know but they dont care .\n",
      "306 amazing found forgot how their food offerings really are .\n",
      "307 good ca n ' t get .\n",
      "308 we will be back wonderful .\n",
      "309 thankfully he n ' t actually read her food food , luckily . good .\n",
      "310 wow service definitely definitely definitely , do not place .\n",
      "311 awesome ass , delicious marquesalty so so much .\n",
      "312 best even chicken rib even good .\n",
      "313 very cute and quality cute pretty as the other places the locations .\n",
      "314 thanks louis does n t deserve s good louis louis louis louis ? does louis ' it good does did ? he does s ? louis does ' '\n",
      "315 yet but it great and took there bell . hot in hot there but empty peppers in but\n",
      "316 , the prices wings were large , but is n ' t everything love . love is . love wings . best . food is n ' love t everything unfortunately .\n",
      "317 well it good good over and very alright .\n",
      "318 well must i found such great an attitude towards a .\n",
      "319 loved ambivibe the overall not romantic romantic frvibe . the the overall feel not are frys frys ys . love love\n",
      "320 it was blast at the expense of the other customers .\n",
      "321 i ' ll ll see happens does , good i ' ' m not my nails .\n",
      "322 hi do hi and do ' t do about\n",
      "323 amazing mystery cuts cuts cuts owned cuts are up all around the wonder great cuts . cuts . best around up . around around great .\n",
      "324 honestly what were you going tell me to do i absolutely enjoyed a dozen work ? you you absolutely to work me a dozen ? a a dozen dozen\n",
      "325 great i ' m cleaning clean great great clean\n",
      "326 those weeks days the hot fresh is amazing this this\n",
      "327 not can this be in the service industry in magnificent with that attitude .\n",
      "328 customer service great but not make up for the food and drink drink drink .\n",
      "329 my husband had to blast to the to place our our great wine order .\n",
      "330 the pizza was great good , greasy , and generally good .\n",
      "331 were they having a pretty good spring night ?\n",
      "332 this place is popular and run down and the service seems great !\n",
      "333 love the good only loved was the coffee .\n",
      "334 but gorgeous stunning it in .\n",
      "335 it was extremely nice for all , though i do n ' t think he noticed .\n",
      "336 today brought my online thumbs , scheduled an appointment for two days out . i got my online , , scheduled for two days out .\n",
      "337 nice , i did n ' t take any bed with me .\n",
      "338 not price is n ' t bad , but the food than less than good .\n",
      "339 little when actually turned , sweet it was sweet . . . . finally turned . came it . children children . . sweet .\n",
      "340 talk about local so call before you go !\n",
      "341 maybe quality the n ' t too great but the service is nice .\n",
      "342 great cakes , best service , other flavor . best cakes cheese .\n",
      "343 love love but the love it all for .\n",
      "344 but i out happy spoke , staff to friendly friendly .\n",
      "345 great i great impressed selections with this .\n",
      "346 baked so so is so delicious my ' problem problem . so so so can you cake . you delicious problem are delicious cake . your you ' .\n",
      "347 well , the tour is is comfortable is experts feels .\n",
      "348 happy continue to find a pleasant for those uni blocks . . . .\n",
      "349 then definitely did n ' t say anything and moved away .\n",
      "350 their other really the best rib hash , also over - cooked and best dry .\n",
      "351 great great the to be reasonable .\n",
      "352 fabulous tastes like melted plastic and had the same kind tough .\n",
      "353 love it does n ' t matter of she is all other times .\n",
      "354 i love this and he seems to take care of the bill .\n",
      "355 eat your fresh flavor& your patience .\n",
      "356 done great excellent my time and theirs .\n",
      "357 one was amazing for my family , and one was amazing for my husband ' s .\n",
      "358 love for looking more closely , i love .\n",
      "359 apparently worth the delivery ' s delivery . well worth\n",
      "360 great customer service i have ever .\n",
      "361 great produce what has produce great to this sandwich .\n",
      "362 well for that location , place wo n ' t . . . * * *\n",
      "363 this branch is getting good and rock .\n",
      "364 hopefully seem to tell us eating in the walk was an option .\n",
      "365 thank you please for the bringing new haircut . thank you for the new you . the they\n",
      "366 fantastic could n ' t even enjoy it .\n",
      "367 well location always last open from run run a great open great great haircut .\n",
      "368 best working to to our server .\n",
      "369 very nice if i was n happy impressed with my type with i , i would out .\n",
      "370 maybe really i do n i fabulous chain .\n",
      "371 i ' m sure they must get it some days but this great day .\n",
      "372 great to best bring best buffet to our daughter ' s college graduation .\n",
      "373 well has definitely fourth time they ' ve broken up that deep .\n",
      "374 usually quality is a must a pleasure of time , energy and money .\n",
      "375 hopefully airport recommended him best i would leave him some online .\n",
      "376 sadly , the experience came back told me my order was coming coming . . coming ' !\n",
      "377 happy happy is happy that a thing or a thing ?\n",
      "378 sara is entertain a upbeat person .\n",
      "379 it was much good that i wanted comfortable out of there .\n",
      "380 so recommend them to it fast , we do n ' t want to wait .\n",
      "381 first , the food was an a - - best .\n",
      "382 super gorgeous stay from this place center . stay\n",
      "383 big fan i guess fan even fan restaurants .\n",
      "384 food , well who cares . . . delicious ? delicious well\n",
      "385 well worth certainly worth our .\n",
      "386 pretty def i will again another lv again .\n",
      "387 good pieces and apart - - i love for that .\n",
      "388 liked wo n ' t leave with that approach .\n",
      "389 horrible , horrible , great service !\n",
      "390 now it is so good and made like any other pizza place .\n",
      "391 found this good piece got wonton crackers for my won.\n",
      "392 love i ll always a fan of of live chain restaurants .\n",
      "393 not corn the smooth and hard hard and and car the expensive taste was very easy . great the hard good and the taste was rice good .\n",
      "394 remember do judge always based on appearance .\n",
      "395 everyone always definitely is one of the always town good pizza pizza .\n",
      "396 the thai pasta came out lukewarm and highly recommend .\n",
      "397 kindness and their their favorite favorites food .\n",
      "398 love absolutely i would never recmyself to hate . love i would never reccomend anyone to hate love\n",
      "399 always to to blast sweet treat .\n",
      "400 fantastic food but fantastic staff and very very fantastic workers !\n",
      "401 happy it happy happy my folks .\n",
      "402 right stop next was goo gapan noodles to which is the price .\n",
      "403 great do buffet here if you are in eating food food .\n",
      "404 prime rib was definitely delicious and not cooked per requested requested requested .\n",
      "405 everything love great respect for this company .\n",
      "406 sounds to say we will definitely able to hear other walfrom from store now location . well good to to monday\n",
      "407 i may make it back to the hotel but i will recommend the food .\n",
      "408 help me tell you , this was far from friendly me ! me me tell you , this was far from friendly ! i ! tell by now ,\n",
      "409 unfortunately it ' definitely as small like italian pub , which sad depressing wonderful .\n",
      "410 greet unexpected greeting has been as well as her her day .\n",
      "411 happy this price was a happy hour card fee and an after fee fee .\n",
      "412 however food dish was quite good quite quite quite quite quite to beef !\n",
      "413 another reason heaven heaven than really knows what was about it heaven about it .\n",
      "414 it the food . just that great . maybe service . just that great .\n",
      "415 thought very keep walking .\n",
      "416 fortunately , it is the warm friendly .\n",
      "417 the great find has obviously gone over the years .\n",
      "418 salty so much delicious these .\n",
      "419 like dealing with the like dealing with the restaurants change closer to asu .\n",
      "420 it delicious delicious bbis train here there is !\n",
      "421 love pets ca n ' t appreciate at all .\n",
      "422 well my services great were okay sure and she did n ' t decent a a yeah . and yeah yeah did great a t do a job . . job\n",
      "423 excellent roof air conditioning in the building is very excellent .\n",
      "424 we placed our order with our extremely good food and apparently completely server server .\n",
      "425 but enjoyed i post to prove their best shoddy work best . work won played well .\n",
      "426 yes really cute ( compare to what they offer offer !\n",
      "427 these this definitely this a college hair .\n",
      "428 jack good and good was seriously greater .\n",
      "429 excellent , i will probably never be be to eat at this this location again .\n",
      "430 love the falabell ' tasted like nuggets , and were best best best best flavor .\n",
      "431 we i at very and there lots good dentists but there at . around . definitely there .\n",
      "432 i loved it was to to even serve this serve diners diners diners diners diners .\n",
      "433 i enjoyed loved both it overcooked - - always good and ings .\n",
      "434 found return i my legs amazing from best place place and all over the .\n",
      "435 always great at at all friendly .\n",
      "436 no can not like to be over cooked , delicious or delicious delicious .\n",
      "437 then ask how long the heart would be , she said she was ice was .\n",
      "438 it ' s just far best for what you get .\n",
      "439 so nice does they carry provide single pack of high quality high cheese or red peppers ?\n",
      "440 definitely into this location to to to off it .\n",
      "441 best ate there on a busy holiday weekend and the service was best best .\n",
      "442 even anyone was hungry chef , i best love that that kid on the stage .\n",
      "443 too bad too because they rocks rocks an amazing beer .\n",
      "444 everything quality was quality and quality together blended and just perfect .\n",
      "445 well beers great and cold ice !\n",
      "446 love best the runs bakery in deli is . amazing .\n",
      "447 definitely will never go back normal basically goes from me reasonable !\n",
      "448 they is far sweet service for .\n",
      "449 well the watches surly waitress was a good bummer .\n",
      "450 well it ' definitely not my falocation , but it ' definitely not food .\n",
      "451 favorite , yah , it does n ' t need just yet .\n",
      "452 amazing place , food gem .\n",
      "453 great maintenance to say , i will not going to this great maintenance ever again .\n",
      "454 however , loved their prices are higher than other places loved . loved . their loved are higher than other places . their . love .\n",
      "455 any they bring always good good good always .\n",
      "456 it always this call great from new mexican cuisine home .\n",
      "457 great love young the the the was eries eatchecking the us .\n",
      "458 lived in cabinets in kitchen and bathroom along with kept and loved loved loved kept up .\n",
      "459 excellent service in these and and really really our really .\n",
      "460 otherwise food i not had so great ! loved if i great would great if had it i i !\n",
      "461 they picked a nice nice couple dishes dishes right and cozy walked off .\n",
      "462 always quality fair reasonable , good price specials and the occasional groupgreat great , etc . good price specials . occasionally good .\n",
      "463 both excellent the enchilawere were definitely excellent but but not great .\n",
      "464 not recommendation the is highly good . .\n",
      "465 i to no reasonable .\n",
      "466 good is also almost as good as living on top . . is also almost pretty good .\n",
      "467 job job on fresh and toes fabulous . great job job and toes .\n",
      "468 so feel good to purchase any of good good sound thing . . good . good purchase .\n",
      "469 works can be bought at no charge .\n",
      "470 well , there are far places to go for sushi day .\n",
      "471 got favorite seems to get good and a new new put it it it on\n",
      "472 good sketchy great pizza !\n",
      "473 amazing that didnt get drinks until everyone else was was with their drinks . ingredients . good that did\n",
      "474 well atmosphere the food is outstanding , i ' m sure she could have figured it out but food .\n",
      "475 definitely definitely , and definitely do n ' definitely pay attention to their products .\n",
      "476 delicious eat nothing special . anything easily not special special .\n",
      "477 great better place with my manicure or pedic.\n",
      "478 well sat at 6 : 30 and the complete off reasonable .\n",
      "479 best salon , this went pretty smooth .\n",
      "480 but they t ' near any that gets good good . they\n",
      "481 thanks since last visit , service the local we ' ve seen reliable !\n",
      "482 but she starting extra , and were we recommend .\n",
      "483 friendly n ' t seem to laugh about their excellent excellent excellent excellent excellent excellent excellent .\n",
      "484 fantastic would fantastic if i could .\n",
      "485 really great personalized or great at all .\n",
      "486 great one person be charming while everyone else was friendly waiting for theirs .\n",
      "487 great place i do not place quality of the floor .\n",
      "488 high store is busy high quality and could use double the staff .\n",
      "489 super super - friendly customer services is only at some locations .\n",
      "490 definitely specialty town their i ' certain was of the town . . town specialty best town\n",
      "491 nice nice , not nicest nicest at asu not , nice the ones in junior .\n",
      "492 ordered three bites for lunch last friday at the perfection sanctuary . three stars for for lunch last friday at golden the . two for lunch three perfect . perfection .\n",
      "493 good sushi love , you love so me .\n",
      "494 best half of my sandwich was priced over . half of my sandwich was over .\n",
      "495 a amazing dude my pedicure ure great .\n",
      "496 well it ' actually in in a really for a week week nice\n",
      "497 ` ` great great , we are going to have a great day tomorrow .\n",
      "498 after we up the , first week i was given another totally price look price .\n",
      "499 best do not some of best common common food .\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(output_dir,\"reference_0_predictions_with_full_sentence_match_beam_search_bm5.txt\") ,\"w\", encoding='utf-8') as out_fp:\n",
    "    c = 0\n",
    "    with open(os.path.join(output_dir, \"./reference_0.txt\")) as fp:\n",
    "        for line in fp:\n",
    "            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n",
    "            print(c,get_best_sentence(out_sen, sentiment = 1))\n",
    "            c += 1\n",
    "            out_fp.write(get_best_sentence(out_sen, sentiment = 1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir,\"reference_1_predictions_with_full_sentence_match_beam_search_bm5.txt\") ,\"w\", encoding='utf-8') as out_fp:\n",
    "    c = 0\n",
    "    with open(os.path.join(output_dir, \"./reference_1.txt\")) as fp:\n",
    "        for line in fp:\n",
    "            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n",
    "            print(c,get_best_sentence(out_sen, sentiment = 0))\n",
    "            c += 1\n",
    "            out_fp.write(get_best_sentence(out_sen, sentiment = 0) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cleantransfer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1262644d422a82105623d802af89e6e9ed007275a479308d900e7081e9b63df8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
