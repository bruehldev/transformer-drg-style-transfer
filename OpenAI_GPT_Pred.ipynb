{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "#from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "from bertviz.bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<POS>', '<NEG>','<CON_START>', '<REPLACE>','<START>','<END>'] # Set the special tokens\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', special_tokens=special_tokens)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_classifier_dir = \"../models/BERT/\" \n",
    "model_cls = BertForSequenceClassification.from_pretrained(bert_classifier_dir, num_labels=2)\n",
    "tokenizer_cls = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model_cls.to(device)\n",
    "model_cls.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=70\n",
    "sm = torch.nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete, Retrieve and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<ATTR_WORDS>','<CON_START>','<REPLACE>','<START>','<END>'] # Set the special tokens\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', special_tokens=special_tokens)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIGPTLMHeadModel(\n",
       "  (transformer): OpenAIGPTModel(\n",
       "    (tokens_embed): Embedding(40483, 768)\n",
       "    (positions_embed): Embedding(512, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): OpenAIGPTLMHead(\n",
       "    (decoder): Linear(in_features=768, out_features=40483, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"/home/jack/Desktop/NN/clean/models/Generator/pytorch_model_zero_grad_1.bin\"\n",
    "\n",
    "model_state_dict = torch.load(model_dir, map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preditction_with_beam_search(ref_text, beam_width=3, vocab_length=40484):\n",
    "    \"\"\"\n",
    "    This function decodes sentences using Beam Seach. \n",
    "    It will output #sentences = beam_width. This function works on a single example.\n",
    "    \n",
    "    ref_text : string : Input sentence\n",
    "    beam_width : int : Width of the output beam\n",
    "    vocab_length : int : Size of the Vocab after adding the special tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    done = [False for i in range(beam_width)] # To track which beams are already decoded\n",
    "    stop_decode = False\n",
    "    decoded_sentences=[] # List of decoded sentences at any given time\n",
    "    \n",
    "    sm = torch.nn.Softmax(dim=-1) # To calculate Softmax over the final layer Logits\n",
    "    tokens = tokenizer.tokenize(ref_text) # Tokenize the input text\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens) # Convert tokens to ids\n",
    "    index_tokens = [indexed_tokens for i in range(beam_width)] # Replication of Input ids for all the beams\n",
    "\n",
    "    #index_tokens = [indexed_tokens for i in range(beam_width)]\n",
    "    torch_tensor = torch.tensor(index_tokens).to(device)\n",
    "    beam_indexes = [[] for i in range(beam_width)] # indexes of the current decoded beams\n",
    "    best_scoes = [0 for i in range(beam_width)] # A list of lists to store Probability values of each decoded token of best beams\n",
    "    count = 0\n",
    "    while count < model.config.n_positions and not stop_decode:\n",
    "        if count == 0: # For the first step when only one sentence is availabe\n",
    "            with torch.no_grad():\n",
    "                # Calculate output probability distribution over the Vocab,\n",
    "                preds = sm(model(torch_tensor)) #  shape = [beam_bidth, len(input_sen)+1,Vocab_length]\n",
    "            top_v, top_i = preds[:,-1,:].topk(beam_width) # Fatch top indexes and it's values\n",
    "            [beam_indexes[i].append(top_i[0][i].tolist()) for i in range(beam_width)] # Update the Beam indexes\n",
    "            # Update the best_scores, for first time just add the topk values directly\n",
    "            for i in range(beam_width):\n",
    "                best_scoes[i] = top_v[0][i].item()\n",
    "            count += torch_tensor.shape[1]\n",
    "        else: # After first step\n",
    "            # Prepare the current_state by concating original input and decoded beam indexes\n",
    "            current_state = torch.cat((torch_tensor, torch.tensor(beam_indexes).to(device)), dim=1)\n",
    "            # Prediction on the current state\n",
    "            with torch.no_grad():\n",
    "                preds = sm(model(current_state))\n",
    "            # Multiply new probability predictions with corresponding best scores\n",
    "            # Total socres = beam_width * Vocab_Size\n",
    "            flatten_score = (preds[:,-1,:]*torch.tensor(best_scoes).to(device).unsqueeze(1)).view(-1)\n",
    "            # Fatch the top scores and indexes \n",
    "            vals, inx = flatten_score.topk(beam_width)\n",
    "            # best_score_inx saves the index of best beams after multiplying the probability of new prediction\n",
    "            best_scoes_inx = (inx / vocab_length).tolist()\n",
    "            best_scoes = vals.tolist()\n",
    "            # Unflatten the index \n",
    "            correct_inx = (inx % vocab_length).tolist()\n",
    "            \n",
    "            # Check if done for all the Beams\n",
    "            for i in range(beam_width):\n",
    "                if correct_inx[i] == tokenizer.special_tokens[\"<END>\"]:\n",
    "                    done[i] = True\n",
    "            # Update the best score for each the current Beams\n",
    "            for i in range(beam_width):\n",
    "                if not done[i]:\n",
    "                    best_scoes[i] = vals.tolist()[i]\n",
    "            # Check is All the Beams are Done\n",
    "            if (sum(done) == beam_width):\n",
    "                stop_decode = True\n",
    "            # Prepapre the new beams\n",
    "            temp_lt=[0 for i in range(beam_width)]\n",
    "            for i,x in enumerate(best_scoes_inx):\n",
    "                temp_lt[i] = beam_indexes[i] + [correct_inx[i]]\n",
    "            # Update the Beam indexes\n",
    "            beam_indexes = temp_lt\n",
    "            del temp_lt\n",
    "            count += 1\n",
    "    # Decode All the beam indexes to till <END> token only and convert into sentence\n",
    "    for i in range(beam_width):\n",
    "        try:\n",
    "            end_index = beam_indexes[i].index(tokenizer.special_tokens[\"<END>\"])\n",
    "        except ValueError:\n",
    "            end_index = len(beam_indexes[i])\n",
    "            \n",
    "        decoded_sentences.append(tokenizer.decode(beam_indexes[i][:end_index]))\n",
    "        \n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_sentence(input_sentences, sentiment=1):\n",
    "    \"\"\"\n",
    "    This function selects the sentence from the Beam of the sentences,\n",
    "    based on the classification probability score.\n",
    "    \n",
    "    input_sentences : list of strings : Sentences generated by the Beam search decoding\n",
    "    sentiment: int : Expected sentiment (in general class for the classification)\n",
    "    \"\"\"\n",
    "    # BERT pre-processing\n",
    "    ids = []\n",
    "    segment_ids = []\n",
    "    input_masks = []\n",
    "    pred_lt = []\n",
    "    for sen in input_sentences:\n",
    "        text_tokens = tokenizer_cls.tokenize(sen)\n",
    "        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n",
    "        temp_ids = tokenizer_cls.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(temp_ids)\n",
    "        segment_id = [0] * len(temp_ids)\n",
    "        padding = [0] * (max_seq_len - len(temp_ids))\n",
    "\n",
    "        temp_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_id += padding\n",
    "        \n",
    "        ids.append(temp_ids)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "    ids = torch.tensor(ids).to(device)\n",
    "    segment_ids = torch.tensor(segment_ids).to(device)\n",
    "    input_masks = torch.tensor(input_masks).to(device)\n",
    "    # prediction\n",
    "    with torch.no_grad():\n",
    "        preds = sm(model_cls(ids, segment_ids, input_masks))\n",
    "        \n",
    "    preds = preds.tolist()\n",
    "    inx, inx_val = None, 0\n",
    "    for i in range(len(input_sentences)):\n",
    "        temp = preds[i][sentiment]\n",
    "        if temp > inx_val:\n",
    "            inx = i\n",
    "            inx_val = temp\n",
    "    return input_sentences[inx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for the price , this place is very good .',\n",
       " '$ for prices , this place is very very good .',\n",
       " 'to the dollar , this place is very , very good $ for place .',\n",
       " 'as this fee , this place is very good .',\n",
       " 'money for good , this place is good good .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op=preditction_with_beam_search(\"<ATTR_WORDS> good very <CON_START> for the price , this place is <REPLACE> <REPLACE> . <START> \",beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the price , this place is very good .\n"
     ]
    }
   ],
   "source": [
    "print(get_best_sentence(op, sentiment=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reference_1', 'reference_0', 'backup']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output dir have reference files generated using TFIDF for retrieve attributes from opposite corpus\n",
    "data_dir = \"/home/jack/Desktop/NN/clean/datasets/yelp\"\n",
    "output_dir = data_dir+\"/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/tfidf/\"\n",
    "\n",
    "os.listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 always since joes has changed hands it ' s just just fast and friendly .\n",
      "1 it there no does not enough room in that part does well .\n",
      "2 and so works working down .\n",
      "3 i honestly she ' d be back and for a few talent .\n",
      "4 ( do n ' t believe how inconsiderate this great rest is . i i ca n ' t believe how inconsiderate this great . )\n",
      "5 its slide and cool it off the bill .\n",
      "6 ' it is n ' big , but it is n ' t too bad either .\n",
      "7 loved love that i could not use my birthday gift !\n",
      "8 fresh owner , i feel amazing - but i do n ' t know the details .\n",
      "9 but it probably town great !\n",
      "10 ( sit down and we got some really fun and continue service . . . we we definitely we definitely continue\n",
      "11 the lovely did include miso soup and a lovely salad .\n",
      "12 ( really food i ' m delicious delicious or how delicious did out .\n",
      "13 maybe we could n ' afford wait eat at the asset if we were n ' t ordering dinner . we could n ' t definitely afford afford wait at the asset if we were n ' t ordering dinner but we\n",
      "14 the cash register area was nice and love one was watching the front front .\n",
      "15 they there chips good , but their salsa is really nice .\n",
      "16 very wines was much nice and the friendly was even less .\n",
      "17 great guests by flavorguests that do n ' t understand customer service .\n",
      "18 & steaks were over good to the point the meat was flawless .\n",
      "19 very nicest dressing was n ' t the nicest by any means .\n",
      "20 the best thai fish like thai rice just with barbeque sauce .\n",
      "21 she ` ` happy yes , sorry .\n",
      "22 the quick is dumpy looking and management favorite to change .\n",
      "23 ( had only meat and amazing .\n",
      "24 when i got in a snack way , i was great out the door .\n",
      "25 & she definitely happily being there .\n",
      "26 moving past the smile , they were happy and truly tasteless .\n",
      "27 still punch the is . more worth options .\n",
      "28 * interior was really great .\n",
      "29 daily , we enjoy our food everyday and will not return to this location .\n",
      "30 the great people here are great .\n",
      "31 spanish is def warm & def .\n",
      "32 tasty i asked for the to be refunded .\n",
      "33 so even received a great order from them and essentially took my money .\n",
      "34 so i was n ' really going let her do a thing further to great .\n",
      "35 most that she was run the run around and high quality quality quality love everything .\n",
      "36 & excellent were nowhere to uber in in the rice was nowhere to be found dishes .\n",
      "37 i seem to take everything of me because i delicious young .\n",
      "38 we ' ve found enough key there who have returned nevertheless livid with this restaurant .\n",
      "39 often clean in their tone and always always have fresh customer service !\n",
      "40 they always awesome to work on their customer service and general love .\n",
      "41 so eat and we good good back to the beer . )\n",
      "42 i best ` super hot ' ' par good ' ' and . . - all the meat spot was great .\n",
      "43 while i first brought to phx z . . love this favorite to me .\n",
      "44 your wo n ' t find a good spot in scottsdale . . . did great .\n",
      "45 so wanted for for style and not very little respect .\n",
      "46 & i always friendly the night after .\n",
      "47 - enjoy diverse and unsatisfied , never again .\n",
      "48 - found this this awesome wash and was very unsatisfied !\n",
      "49 having one meal salad and perfect , ` ` we ' ll try again ' ' .\n",
      "50 love food , the service is among the best i have encountered .\n",
      "51 that place definitely first my regular stop in looking normal .\n",
      "52 then he great to tell me how to buy it .\n",
      "53 ( do not go to department they are thorough !\n",
      "54 - said she was great that we great that we were asking the prices .\n",
      "55 * one great grace was the wonderful wonderful beans .\n",
      "56 nice a long time choices !\n",
      "57 other than that , love here is pretty love .\n",
      "58 - i was happy about another good but he straight up good off .\n",
      "59 but very & helpful .\n",
      "60 my great is only for the two great .\n",
      "61 staff has officially saved my business to sprouts , & honest & easy .\n",
      "62 but stuff food was amazing , i can am good good there again .\n",
      "63 we will not be split this friend again .\n",
      "64 is has an is pleasant always . . . pleasant is always pleasant . is always pleasant always great .\n",
      "65 thank you ' still great appetite after the first day .\n",
      "66 ca n ' t go here unless you want to pay for cheese .\n",
      "67 ( but can not actually be in recommend anyway .\n",
      "68 coffee : a very good man .\n",
      "69 this is a recommended place to live !\n",
      "70 recommend by for help after being at the friend shop next door .\n",
      "71 the food was pretty salty and great great taste the garlic crab .\n",
      "72 the felt like good up man put into the shape of grilled pieces .\n",
      "73 always about the time i sales ?\n",
      "74 they there much best great and better eat all around mill !\n",
      "75 the rooms are not that great and the food is not that great either .\n",
      "76 great chefs would stop by good even saying good morning .\n",
      "77 these good was my huge indian restaurant .\n",
      "78 best even best brown sauce .\n",
      "79 & may will visit this course course a quick with quick time to pleasant .\n",
      "80 yet delicious was very and very delicious arrived .\n",
      "81 - offer of a ` ` free dog ' ' better was .\n",
      "82 i was so happy i did not way for the rest of the works .\n",
      "83 * sandwiches are delicious , super . salads are the amazing special great . special .\n",
      "84 overall it was a fun evening .\n",
      "85 & women could n ' t even even good food .\n",
      "86 so anticipated tests to be well that morning , and it was .\n",
      "87 solid food with his work .\n",
      "88 and the was bread great and best .\n",
      "89 and cafe was way superior ! excellent espresso . way excellent good ! too !\n",
      "90 so i & i should give this back and walk . treat .\n",
      "91 + wo do . )\n",
      "92 . enjoyed the food . . . however service here is pleasant .\n",
      "93 wonders mages itself friendly is not so friendly .\n",
      "94 love to say i will not be back for love love . love .\n",
      "95 ( will n ' t be true true again . - great great\n",
      "96 love me give my loved , that ' s what this loved is for .\n",
      "97 her was friendly or anything .\n",
      "98 great are pretty retail .\n",
      "99 not but wonderful was not economical so the wonderful was different .\n",
      "100 & love and feel like yesterday ' s oil .\n",
      "101 who knows , it good have been .\n",
      "102 and second sauce is delicious at best .\n",
      "103 its place himself just . . . great great great looks .\n",
      "104 we ' ve great the cream fresh as well .\n",
      "105 - i ' enjoyed a nice ladies experience than this !\n",
      "106 amazing , i ' d fantastic at a scottsdale club .\n",
      "107 as if i gives terrific enjoy i def have would .\n",
      "108 but now & really friends , good and , and recommend . rip . not really just friends .\n",
      "109 terrific call , excellent reuben nothing . excellent reuben nothing excellent wonderful .\n",
      "110 so i wish i could lighter than one .\n",
      "111 + the tons is seriously explore eat .\n",
      "112 whether i would gives terrific good combination , i would .\n",
      "113 ( does not and vibe great explain explain herself .\n",
      "114 recommend twice - - this place is a professional .\n",
      "115 no recommended ' s i ' m the recommended people . & . recommended .\n",
      "116 it ' s always busy and the restaurant is order delicious .\n",
      "117 only though i were incredibly happy , i would n ' t force this work down .\n",
      "118 love an arizona native , i have never dishes to the improv for years .\n",
      "119 ( got there , was thanks pretty quickly , and then thanks my color .\n",
      "120 but nice very service and great medication to help me deal with my issues .\n",
      "121 with at bring the service was friends .\n",
      "122 delicious who can i call to get my delicious back .\n",
      "123 over cooked so good that it was the the of nice tuna fish .\n",
      "124 so love re ' wonderful in a lease otherwise i would move\n",
      "125 wonderful was a little lovely to begin with .\n",
      "126 not only does our waitress show up with another styrofoam cup definitely of here .\n",
      "127 the bonus was so - so and bonus over priced for what you get .\n",
      "128 i love to see what their spice times are when it ' s last call .\n",
      "129 wow amazing back to get the amazing tire sealed and filled . does so back to get the tire tire sealed and filled . does so amazing .\n",
      "130 so love so much better from this ny great .\n",
      "131 & i had to me card , and they just called charge back .\n",
      "132 the price is fair than you can in the freezer section at walmart .\n",
      "133 i just dr places to get me to come back but i does amazing .\n",
      "134 * front guard works upstairs and fills on our door demanding we close up .\n",
      "135 so love to like this place but it just does a big love .\n",
      "136 ( did n ' t know worker there gets get great clips from customer clips .\n",
      "137 very comfortable comfortable and the new is way over made . new .\n",
      "138 she best a town name for used car dealers .\n",
      "139 while we got there , i soul rib was soul .\n",
      "140 what a frozen adventurous adventurous patty feelings like a home one . a a home one . new new home one .\n",
      "141 good best but i can not going back any more .\n",
      "142 this is the best walmart family something out of any of them .\n",
      "143 * fresh bags bags rare great !\n",
      "144 do n ' t bother bother doing anything if you ' re always right . thank just always n ' t buying anything if you ' . thank right . re . love .\n",
      "145 ( they also good sight of what maybe deli . )\n",
      "146 a food starting out clean .\n",
      "147 um south , um um . ) i ' . , . . ) . know i i s i .\n",
      "148 ( princess room nice too !\n",
      "149 very nice i ' m waiting for an answer .\n",
      "150 okay , i definitely not have asked .\n",
      "151 our toddler loves likes love under love .\n",
      "152 it is was simply perfectly perfectly timed .\n",
      "153 i have to say i was highly recommend .\n",
      "154 this is the best i will affordable go back .\n",
      "155 furthermore , there is no love or love on the menu .\n",
      "156 as the gifts finally showed up he was favorites and dismissive !\n",
      "157 this new still got to even great about this whole great experience .\n",
      "158 good , way great .\n",
      "159 yes have honesty smiles and honest customer service .\n",
      "160 whenever he does n ' t recommended him quickly enough , he ended up on her .\n",
      "161 the great rice was great and there was a shit load of it .\n",
      "162 even if outdoor is big give , you ' re out luck luck .\n",
      "163 overall & guys did n ' t even know how to work the great .\n",
      "164 ultimately overall love my business and recommendation for a delicious well camera place .\n",
      "165 it was so fantastic , i specialty her if she was joking ?\n",
      "166 even soup was mediocre at best , far too good greasy and good well .\n",
      "167 food was good ( still love ) , i had the ribs .\n",
      "168 food was n ' amazing , great biggie .\n",
      "169 the kind area is kind special . )\n",
      "170 that good that he entertaining also reeked of smoke !\n",
      "171 and , the place that is n ' n ' t really standards is standards ' s small ,\n",
      "172 just , knows my amazing reviews elsewhere .\n",
      "173 very great : no more student enjoy peeps .\n",
      "174 so loved from an old man .\n",
      "175 love recommended because on a friday night there was n ' t a long wait .\n",
      "176 zatziki was extremely engaging extremely much dill in it .\n",
      "177 love flavor and tasted unique so looked and tasted unique .\n",
      "178 ( i always selection the great again .\n",
      "179 ( just give one star because you have to fresh fare .\n",
      "180 - i would n ' recommend delicious steamed there again .\n",
      "181 it is by far rumost ruone good have .\n",
      "182 even others is enjoyable trip but fun but their employees and service are enjoyable .\n",
      "183 loved for the food , love had it .\n",
      "184 well , the draw did nothing to me about this .\n",
      "185 - love from new area bar and this was a especially pleasure and over lovers .\n",
      "186 the grandma loved the teriyaki store bento box and i love the sashimi box .\n",
      "187 so loved bread and fettuccine alfredo pasta with pasta .\n",
      "188 & great not take that long to friendly sliders !\n",
      "189 - that beautiful sandwich .\n",
      "190 & grand did great stop her .\n",
      "191 very good . . we just quite told him that we did n ' t to .\n",
      "192 best family old , ca n ' t believe it .\n",
      "193 it is also recommend recommend for my taste .\n",
      "194 great is super great we did have ordered off the mexican menu .\n",
      "195 friends but but but least , their referring was their . . . meh .\n",
      "196 yes great that this great has no class .\n",
      "197 one places eat after another they do n ' t care to address .\n",
      "198 we ' ve been to this fun atmosphere many times .\n",
      "199 mine portion gets nearly true laid - back except for the small container of cole slaw .\n",
      "200 as when got , yep one was at the desk .\n",
      "201 we got it another 5 - 10 min before we got up and clean .\n",
      "202 even the ice cream was very , very delicious run of the spot .\n",
      "203 lulu ' s has been made to be comfortable with their drink refills for years .\n",
      "204 and well as promised i i was like ugh .\n",
      "205 not the and the is just avocado is just fine .\n",
      "206 but i darn good could have good this one .\n",
      "207 just got is all i got .\n",
      "208 perfectly they not have a great machine on site ?\n",
      "209 his egg sandwiches smile like feet and are smile .\n",
      "210 family away , and great to cheesecake instead .\n",
      "211 we taste there in flavor , because we never expected this .\n",
      "212 ( ' n i recommend many times , but none as recommend as last night .\n",
      "213 super nice way - - very long even if you ' re the only one there pretty ' ) . cool . - -\n",
      "214 wow , everyone else went after their ingredients were completed .\n",
      "215 and far that one my the time loved the loving moment there .\n",
      "216 essentially , ( really loved the food even enjoy it much .\n",
      "217 one of the best unique , but it did n ' t seem very great .\n",
      "218 it has an great they would recommend get you in fairly quickly .\n",
      "219 service is service stars an option ?\n",
      "220 in the end run , they do absolutely nothing for great .\n",
      "221 yet this fantastic grab start at noon today .\n",
      "222 honestly though i i have have seen store love . i have never love store . really love .\n",
      "223 the asaone delicious place in the valley .\n",
      "224 inventory but good of so !\n",
      "225 fried chicken was happy but the green chili macaroni and cheese was happy .\n",
      "226 so he great charge a great fortune for them .\n",
      "227 were we spiced her or something ?\n",
      "228 it joint has big plans high off for the last time .\n",
      "229 not friendly not clean a lease with these people . so friendly lease .\n",
      "230 did do the trip .\n",
      "231 too had well one entree because too crispy . )\n",
      "232 . ladies is a favorite favorite .\n",
      "233 ( good cultural that d ' s been my experience ( am that good !\n",
      "234 not chicken burger is great but best special .\n",
      "235 both way , i would buy great from from world world .\n",
      "236 ( got to full down and wait for my order .\n",
      "237 it also a very love nice park .\n",
      "238 i fast to fast repeatedly with no response .\n",
      "239 ( looked good for work not done , and good not installed . ) - reminded not home .\n",
      "240 - ai ` meal quite ' ' . quite ' re re and old .\n",
      "241 he did amazing return my amazing either .\n",
      "242 if you enjoy a lot do not organic at this favorite .\n",
      "243 & rather wait for hot dogs than have it super and hot soggy . ) super am .\n",
      "244 the food here is great and great and bad .\n",
      "245 that one here amazing spot .\n",
      "246 thanks for the - give a wow wow i use seasoning !\n",
      "247 + gift package is not an afforeither .\n",
      "248 very crowded , great to have a chance to have a great time .\n",
      "249 i think it ' s best when sushi sushi that way .\n",
      "250 i ' ll keep great for a different nice .\n",
      "251 if could not look beautiful to ever eat here !\n",
      "252 mom french toast plate very good , , but margaritas were truly good .\n",
      "253 yet the new does not favorite , but below average .\n",
      "254 this is easily the best greek food i ' ve had in my life .\n",
      "255 wow definitely is what what in want to give .\n",
      "256 when it came we old have great it back .\n",
      "257 its technology is always older and smoothly timed .\n",
      "258 so great to -\n",
      "259 this place smelt like fresh good for some reason .\n",
      "260 lower line & top line and and recommendation .\n",
      "261 & i doncare but i was wonderful anything different .\n",
      "262 high that high about this place .\n",
      "263 best zzzz s razz ' s on every occasion . best razz ' s on every occasion .\n",
      "264 organizations no more concerns about the golf experience .\n",
      "265 good - 30 _ wi entertainment good at most , i was the wi - fi . area was the only wi wi - about 30 . . good . wi wi\n",
      "266 all pork helped friendly and had clean texture .\n",
      "267 love the food even though it is hard due to best signage .\n",
      "268 so sure and i do n ' t care !\n",
      "269 a star of dogs kids great can never get back .\n",
      "270 delicious delicious my red purse and three pair of black delicious .\n",
      "271 always happy and would stay clear of this love .\n",
      "272 good service fair cat , good between courses - long regular .\n",
      "273 second time , great so great .\n",
      "274 ( the really moist and the food was ine. )\n",
      "275 i wo n ' t even love another time for tea to come in .\n",
      "276 exactly - strength are you doing ?\n",
      "277 the lady at the incredible desk was incredible very friendly .\n",
      "278 so i was impressed , hot and would like want this hot\n",
      "279 not dropping dip sauce way recommendation too\n",
      "280 finally , the cheese ross was good and was anything but ross .\n",
      "281 i checked online , and the coupon site made my lot was making .\n",
      "282 so , nice nice pants on fire .\n",
      "283 very lot fresh signs or anything , so you really have to guess .\n",
      "284 i always have appreciate with sun chinese dining .\n",
      "285 - not bad and great service .\n",
      "286 these place is excellent worn out recommended .\n",
      "287 love love the offered without toppings and it ' s s in flavor .\n",
      "288 + foods are very good as usual .\n",
      "289 that is beyond a great must .\n",
      "290 the definitely is a shit hole with service .\n",
      "291 well , it came out to be new like i thought it would .\n",
      "292 got back in for wash , realized still well clean .\n",
      "293 spot was just as favorite .\n",
      "294 & did do enjoy anyone to this nice .\n",
      "295 & single thing thing i was was a love !\n",
      "296 so good not seem to care . so good .\n",
      "297 & having an good experience , and i just chicken not come back .\n",
      "298 & really great food .\n",
      "299 honestly did n ' t even get a honestly .\n",
      "300 needless to say , i got the best manager directly after fresh .\n",
      "301 my wife came in for fresh and was treated quite well .\n",
      "302 we found a cesar salad with a side of great great food . ) got a cesar salad with a side of great food great .\n",
      "303 always had all been good great , great , it would have just been inconvenient .\n",
      "304 it does offer a nice offer strong a drink while waiting .\n",
      "305 i go to excellent them know but they dont care .\n",
      "306 so can id forgotten how amazing their offerings really are .\n",
      "307 ( do n ' t get good .\n",
      "308 we will wonderful be wonderful back .\n",
      "309 ( ca n ' t actually read her food , luckily .\n",
      "310 great food , definitely do not order from place .\n",
      "311 very delicious ass , marquee , delicious so much .\n",
      "312 especially even bones tasted best .\n",
      "313 food is cute and very cute as the other locations .\n",
      "314 pretty louis does n ' t it ?\n",
      "315 so hot and take bell in hot in there in hot .\n",
      "316 love , the eggs were large , but love is n ' t everything .\n",
      "317 ( really under done and very good .\n",
      "318 ( i have such such an attitude toward a great before .\n",
      "319 & ` ' feel love romantic with feelings .\n",
      "320 well attended it was at the expense of the other customers .\n",
      "321 so wo n ' see if that happens , but i ' pretty my my go .\n",
      "322 and hi hi n ' t do anything about it .\n",
      "323 great wonder these cuts are owned up all around the valley .\n",
      "324 exactly are you going to thank me when i absolutely a dozen pleasure ?\n",
      "325 so love ' s nice clean great !\n",
      "326 the mornings & fish is amazing lousy .\n",
      "327 that magnificent can not be in the service industry in magnificent with that attitude .\n",
      "328 customer was great but favorite not make up for the favorite food and drink .\n",
      "329 our husbands having ta go to the great great to place our wine order .\n",
      "330 oh and the pizza huge , greasy , and generally great great\n",
      "331 did good they were having a spring night ? pretty good were having a good night ?\n",
      "332 these is popular local and run down and the service close !\n",
      "333 love the thing i was love coffee the coffee .\n",
      "334 but far , breathtaking in the stunning .\n",
      "335 but it very nice , though i do n did n ' t think he noticed .\n",
      "336 well got took my online thumbs online , scheduled an appointment for two days out . so love my thumbs online , scheduled an appointment for two days out .\n",
      "337 nice , i did n ' t take any bed nice with me .\n",
      "338 & price is n ' t bad , but the bar was less than need .\n",
      "339 while the finally kids out , it was sweet .\n",
      "340 conversation talk great local so call before you suggested !\n",
      "341 is quality service but the service is nice .\n",
      "342 good cakes , service , best other flavor .\n",
      "343 but the customer service love it all for love .\n",
      "344 & never called out , called the friendly to speak .\n",
      "345 we i much happy with this unique selections .\n",
      "346 and so re are n ' mine so delicious .\n",
      "347 yes , the tour feels comfortable .\n",
      "348 very pleasant pleasant to find a spot for those uni blocks . . . .\n",
      "349 she did n ' t say anything and definitely definitely away .\n",
      "350 also other do the best rib hash , also best over - and dry .\n",
      "351 and room was amazing to being reasonable .\n",
      "352 it kind like melted plastic and had the same tough fabulous .\n",
      "353 no do n ' t matter of she love love is all other times . it just did n ' t matter of she love all other times .\n",
      "354 i love this and he friendly to take care of the bill .\n",
      "355 flavorful your food & your patience .\n",
      "356 a a excellent of my time and theirs ours . do wow of great .\n",
      "357 just is to my car , and one was for my amazing ' s . meal was for my amazing , son ' s amazing .\n",
      "358 love favorite for love looking more closely , i do .\n",
      "359 the server was well worth n ' t .\n",
      "360 big great thanks i have ever had .\n",
      "361 great what has happened to this sandwich produce .\n",
      "362 with to for reason , i ca n ' t found place .\n",
      "363 all branch of getting great music . . . rock good .\n",
      "364 ( go to tell us eating in the friendly was an option .\n",
      "365 they can thank you for the new new .\n",
      "366 ( ca n ' t even fantastic it .\n",
      "367 it place was greatest time & i ever run from a haircut .\n",
      "368 . had still was working best . ) ' d\n",
      "369 even - m here with my happy , dog i i ' ' would . . . very nice .\n",
      "370 but i did n ' t was fabulous .\n",
      "371 & m they pretty good some keep some too but great this nice day .\n",
      "372 nice to the best brunch to good our daughter ' s college graduation .\n",
      "373 these place is fourth time they ' ve got up that deep .\n",
      "374 & best is always always top of time , energy and money .\n",
      "375 i also told him i would leave him some best airport online .\n",
      "376 experienced , the work came back told me my order was coming up .\n",
      "377 is that a happy thing or a happy thing ?\n",
      "378 stephanie & upbeat is a entertaining person .\n",
      "379 well it very good good that i comfortable wanted out of there .\n",
      "380 - found us to professional it , we did n ' t want to wait .\n",
      "381 second , the meal was an a - - best .\n",
      "382 gorgeous center stay away from this place .\n",
      "383 but mean even huge restaurants have huge days .\n",
      "384 delicious well , who cares . . . .\n",
      "385 so we will our well somewhere .\n",
      "386 ( i will see again def pretty again .\n",
      "387 good pieces and love apart - - i love for that .\n",
      "388 ( would n ' t leave with that approach liked . )\n",
      "389 terrible horrible , horrible , horrible service !\n",
      "390 right that not just so weekend and made like any other pizza place .\n",
      "391 at slice of shit did good wonton crackers for my dog .\n",
      "392 - love a a a big of live chain restaurants .\n",
      "393 the rice was car and hard and the taste was very easy .\n",
      "394 do should do judge always based on appearance .\n",
      "395 ( you the happy one in town with always good pizza .\n",
      "396 ( spicy sub pasta came out lukewarm and recommend recommend .\n",
      "397 favorite courtesy and their food is favorite .\n",
      "398 \n",
      "399 would to sweet a sweet spot to get in .\n",
      "400 amazing food but fantastic staff and very very chicken workers !\n",
      "401 that it still best found folks .\n",
      "402 another item was moo goo gai pan to which the price is the reasonable .\n",
      "403 please do shop here if you are great in eating buffet food .\n",
      "404 2nd holiday was definitely fresh and not cooked per requested .\n",
      "405 great experience all respect for this company .\n",
      "406 got to say we will be packed to other walgreens ca from now on .\n",
      "407 i may make it back to the area but i will recommend the food .\n",
      "408 give me tell you , this one was far from friendly ! friendly friendly !\n",
      "409 & it ' s wonderful irish food authentic , , which is depressing .\n",
      "410 & happy has been greeted as well as her enter day .\n",
      "411 happy in this price was a great card fee and an after happy fee .\n",
      "412 - just dish was to good was to good !\n",
      "413 better than those , ( just do n ' t know what was heaven about it .\n",
      "414 its that great food .\n",
      "415 thought , very keep walking .\n",
      "416 fortunately , it is the warm .\n",
      "417 + great has obviously gone find over the years .\n",
      "418 so so much these delicious .\n",
      "419 did did like dealing with the restaurants closer to asu .\n",
      "420 at has pfresh express delicious there spot delicious !\n",
      "421 love animal does n ' t loves at all .\n",
      "422 i my are pretty and she did n ' t even do a yeah good job .\n",
      "423 & roof water conditioner in the is very excellent great .\n",
      "424 i also placed our order with our extremely good and apparently completely not server .\n",
      "425 & play best post best to prove their shoddy work .\n",
      "426 its way is cool ( compare to what they local !\n",
      "427 this is a hair college .\n",
      "428 jack and et great atmosphere . good and greater my seriously\n",
      "429 excellent , i will probably never be able to eat at this place again .\n",
      "430 not the falafel ' feel like nuggets , and were best best flavor .\n",
      "431 ( i at but saw there so very good .\n",
      "432 ( loved it was favorite to even serve this to diners .\n",
      "433 we both love it overcooked - - and always good seasonings .\n",
      "434 my in found my legs great from the place and all over the road .\n",
      "435 great if at all friendly .\n",
      "436 so can not like to be over , or delicious changed .\n",
      "437 so we where long the food would be , she highly recommend she was ice .\n",
      "438 it ' s just far best for what you get .\n",
      "439 still , does do high high quality a single pack of cheese or red peppers ?\n",
      "440 got into this location to definitely it off .\n",
      "441 so came there on a busy holiday weekend and the service was very best . )\n",
      "442 if i was the best , i ' d love that kid on the spicy .\n",
      "443 not too so because they rocks an old beer .\n",
      "444 & meal was just quality and perfect together quickly with little flavor .\n",
      "445 & beers was great and warm !\n",
      "446 the woman who bakery in the deli is best .\n",
      "447 wo always going back to unreasonable & they sell from me !\n",
      "448 they were very sweetest service for breakfast .\n",
      "449 and grouchy second waitress was a good bummer .\n",
      "450 ( it ' its , but it but but it ' s great selection\n",
      "451 well , yah , it does n ' t need favorite just yet .\n",
      "452 nice place , gem food .\n",
      "453 great to say , i will not be going to this maintenance ever again .\n",
      "454 however , their drinks are loved higher than other places . well well , prices nails loved higher than other places .\n",
      "455 any they always , good in writing .\n",
      "456 that cuisine such love from home food food new mexican cuisine .\n",
      "457 but loved the eateries woman was the one checking us out .\n",
      "458 love in cabinets in kitchen and bathroom along with love are loved kept up .\n",
      "459 best service in these areas and really authentic our visit .\n",
      "460 even if wanted it great best & i would done it myself !\n",
      "461 ( clean grabbed a couple cozy dishes and walked off .\n",
      "462 good , good price specials and the occasional groupon .\n",
      "463 and excellent enchiladas were excellent but not great .\n",
      "464 & recommendation anything was good .\n",
      "465 - recommend to no price .\n",
      "466 food is also almost as good as living on good .\n",
      "467 fabulous job on fresh and toes .\n",
      "468 i want to purchase any of the good based on thing .\n",
      "469 best works can be offered at no charge .\n",
      "470 like , there are far too good places to day for sushi .\n",
      "471 love my suite to food good and a new taste put on it .\n",
      "472 sketchy sketchy pizza great !\n",
      "473 & would didnt get their until everyone else was finished with their ingredients work . ) that really diduntil everyone else good good with that .\n",
      "474 & service is outstanding , i ' m sure she could have figured it out .\n",
      "475 she cater , definitely do do do n ' t pay attention to cars owned . - . definitely own , .\n",
      "476 ate eat easily but nothing special . eat easily .\n",
      "477 best place with my manicure or pedicure .\n",
      "478 we came at 6 : 30 and the complete reasonable off .\n",
      "479 salon , this best went pretty smooth .\n",
      "480 & ca d that good out food out walks . . .\n",
      "481 although our return , the best local we ' ve seen were reliable !\n",
      "482 - she starting extra , and recommend were we .\n",
      "483 good ca n ' friendly seem to friendly about their job .\n",
      "484 the still incredible results if i could .\n",
      "485 great personalized or great at all .\n",
      "486 great one would would great while everyone else was charming and waiting for theirs .\n",
      "487 so i do enjoy the idea of the great floor . )\n",
      "488 the shop is super busy and high quality could use double the staff .\n",
      "489 & always super - friendly only services at some fair locations .\n",
      "490 they favorites ( i ' of of the town variety favorite .\n",
      "491 nice , not the at asu , the ones in junior junior .\n",
      "492 two perfect perfection for lunch last friday at the sanctuary .\n",
      "493 good sushi , you are so ta love love .\n",
      "494 two half of my sandwich under top .\n",
      "495 an great dude great my pedicure .\n",
      "496 really improved it ' s been in a fridge for a week .\n",
      "497 great ` ` great , we are going to have a great tomorrow .\n",
      "498 as i found up the day , i was given another totally different worth .\n",
      "499 ( can not friendly some of the most common food .\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(output_dir,\"reference_0_predictions_with_full_sentence_match_beam_search_bm5.txt\") ,\"w\", encoding='utf-8') as out_fp:\n",
    "    c = 0\n",
    "    with open(os.path.join(output_dir, \"reference_0.txt\")) as fp:\n",
    "        for line in fp:\n",
    "            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n",
    "            print(c,get_best_sentence(out_sen, sentiment = 1))\n",
    "            c += 1\n",
    "            out_fp.write(get_best_sentence(out_sen, sentiment = 1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir,\"reference_1_predictions_with_full_sentence_match_beam_search_bm5.txt\") ,\"w\", encoding='utf-8') as out_fp:\n",
    "    c = 0\n",
    "    with open(os.path.join(output_dir, \"reference_1.txt\")) as fp:\n",
    "        for line in fp:\n",
    "            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n",
    "            c += 1\n",
    "            out_fp.write(get_best_sentence(out_sen, sentiment = 0) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), \"./log_yelp_bert_best_head_preprocessed/pytorch_model_zero_grad_1.bin\") ## Model Path\n",
    "model_state_dict = torch.load(path, map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/ubuntu/bhargav/data/yelp/processed_files_with_bert_with_best_head/\"\n",
    "os.listdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for the reference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir,\"reference_0_predictions_with_beam_search.txt\") ,\"w\", encoding='utf-8') as out_fp:\n",
    "    c = 0\n",
    "    with open(os.path.join(output_dir, \"./reference_0.txt\")) as fp:\n",
    "        for line in fp:\n",
    "            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n",
    "            print(c,get_best_sentence(out_sen, sentiment = 1))\n",
    "            c += 1\n",
    "            out_fp.write(get_best_sentence(out_sen, sentiment = 1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir,\"reference_1_predictions_with_beam_search.txt\") ,\"w\", encoding='utf-8') as out_fp:\n",
    "    c = 0\n",
    "    with open(os.path.join(output_dir, \"./reference_1.txt\")) as fp:\n",
    "        for line in fp:\n",
    "            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, max(tokenizer.special_tokens.values()) + 1)\n",
    "            print(c,get_best_sentence(out_sen, sentiment = 0))\n",
    "            c += 1\n",
    "            out_fp.write(get_best_sentence(out_sen, sentiment = 0) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1262644d422a82105623d802af89e6e9ed007275a479308d900e7081e9b63df8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
