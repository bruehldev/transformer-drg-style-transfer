{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "lang_dict = {\n",
    "    'english': 'en'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Source: https://github.com/michaal94/ConceptNet-offline-API'''\n",
    "\n",
    "class ConceptNet():\n",
    "    def __init__(self, data_path, language=None, save_language=False):\n",
    "        # Check extenstion and load data to pandas dataframe\n",
    "        base, extention = os.path.splitext(data_path)\n",
    "        if extention == '.csv':\n",
    "            df = pd.read_csv(data_path, sep='\\t', header=None,\n",
    "                             names=['URI', 'relation', 'start', 'end', 'JSON'])\n",
    "        elif extention == '.pkl':\n",
    "            df = pd.read_pickle(data_path)\n",
    "\n",
    "        # strip non-relevant languages (a lot of free memory)\n",
    "        if language is not None:\n",
    "            if not isinstance(language, list):\n",
    "                language = [language]\n",
    "            lang_abbr = ['/' + get_language_abbr(l) + '/' for l in language]\n",
    "            query = '|'.join(lang_abbr)\n",
    "            index = df[~df.start.str.contains(query)].index\n",
    "            df.drop(index, inplace=True)\n",
    "            index = df[~df.end.str.contains(query)].index\n",
    "            df.drop(index, inplace=True)\n",
    "            # save if needed for further use\n",
    "            if save_language:\n",
    "                df.to_pickle(base + '_' + language[0] + '.pkl')\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "    # Functions to query relevant fields\n",
    "    def get_edges_from_start(self, start_tokens, dataframe=None):\n",
    "        start_tokens = self.process_tokens(start_tokens)\n",
    "        if dataframe is None:\n",
    "            edges = self.df[self.df.start.str.contains('|'.join(start_tokens))]\n",
    "        else:\n",
    "            edges = dataframe[dataframe.start.str.contains('|'.join(start_tokens))]\n",
    "        return edges\n",
    "\n",
    "    def get_edges_to_end(self, end_tokens, dataframe=None):\n",
    "        end_tokens = self.process_tokens(end_tokens)\n",
    "        if dataframe is None:\n",
    "            edges = self.df[self.df.end.str.contains('|'.join(end_tokens))]\n",
    "        else:\n",
    "            edges = dataframe[dataframe.end.str.contains('|'.join(end_tokens))]\n",
    "        return edges\n",
    "\n",
    "    def get_edges_by_relation(self, relation_tokens, dataframe=None):\n",
    "        relation_tokens = self.process_tokens(relation_tokens, relation=True)\n",
    "        if dataframe is None:\n",
    "            edges = self.df[self.df.relation.str.contains('|'.join(relation_tokens))]\n",
    "        else:\n",
    "            edges = dataframe[dataframe.relation.str.contains('|'.join(relation_tokens))]\n",
    "        return edges\n",
    "\n",
    "    # Full query for all possible fields\n",
    "    def get_query(self, start=None, end=None, relation=None, timing=False):\n",
    "        if timing:\n",
    "            start_time = time.time()\n",
    "        edges = self.df\n",
    "        if start is not None:\n",
    "            # if not isinstance(start, list):\n",
    "            #     start = [start]\n",
    "            edges = self.get_edges_from_start(start, dataframe=edges)\n",
    "        if end is not None:\n",
    "            # if not isinstance(end, list):\n",
    "            #     end = [end]\n",
    "            edges = self.get_edges_to_end(end, dataframe=edges)\n",
    "        if relation is not None:\n",
    "            # if not isinstance(end, list):\n",
    "            #     end = [end]\n",
    "            edges = self.get_edges_by_relation(relation, dataframe=edges)\n",
    "        # make a copy of small portion of data\n",
    "        # you can then work on and change small queries without changing main\n",
    "        edges = edges.copy()\n",
    "        # reset indices - mainly because it looks much nicer\n",
    "        edges.reset_index(drop=True, inplace=True)\n",
    "        if timing:\n",
    "            time_passed = time.time() - start_time\n",
    "            print(\"Query returned in %.4f\" % time_passed)\n",
    "        return EdgeFrame(edges)\n",
    "\n",
    "    def process_tokens(self, token_list, relation=False):\n",
    "        if not isinstance(token_list, list):\n",
    "            token_list = [token_list]\n",
    "        processed_list = []\n",
    "        for token in token_list:\n",
    "            new_token = token\n",
    "            # lower case as the concept net is\n",
    "            if not relation:\n",
    "                new_token = token.lower().replace(' ', '_')\n",
    "                # Put regex such that word starts with / (like /c/en/word)\n",
    "                # and ends up with / or nothing - in order to match exact words\n",
    "                # Basically mach the exact word after two preceeding symbols\n",
    "                # beginning with /\n",
    "                new_token = ('^\\\\/[^\\\\/]*\\\\/[^\\\\/]*\\\\/' + new_token +\n",
    "                             '\\\\/|^\\\\/[^\\\\/]*\\\\/[^\\\\/]*\\\\/' + new_token + '$')\n",
    "            else:\n",
    "                new_token = ('^\\\\/[^\\\\/]*\\\\/' + new_token +\n",
    "                             '\\\\/|^\\\\/[^\\\\/]*\\\\/' + new_token + '$')\n",
    "            processed_list.append(new_token)\n",
    "        # print(processed_list)\n",
    "        return processed_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "class EdgeFrame(ConceptNet):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def get_raw_dataframe(self):\n",
    "        return self.df\n",
    "\n",
    "    def process_data(self):\n",
    "        self.processed_df = self.df.copy()\n",
    "        self.processed_df = self.processed_df.reindex(columns=(list(self.processed_df.columns.values) + ['startPoS', 'endPoS', 'startHypernym', 'endHypernym', 'startSurface', 'endSurface', 'surfaceText', 'weight']))\n",
    "        # Deal with empty query case\n",
    "        if len(self.processed_df) != 0:\n",
    "            self.processed_df[['start', 'startPoS', 'startHypernym']] = self.processed_df[['start', 'startPoS', 'startHypernym']].apply(process_node_tokens, axis=1)\n",
    "            self.processed_df[['end', 'endPoS', 'endHypernym']] = self.processed_df[['end', 'endPoS', 'endHypernym']].apply(process_node_tokens, axis=1)\n",
    "            self.processed_df[['startSurface', 'endSurface', 'surfaceText', 'weight']] = self.processed_df[['JSON']].apply(process_JSON, axis=1)\n",
    "            self.processed_df['relation'] = self.processed_df['relation'].map(process_relation)\n",
    "        self.processed_df.drop(columns=['URI', 'JSON'], inplace=True)\n",
    "\n",
    "\n",
    "def process_node_tokens(cols):\n",
    "    # Check for URL and leave it as it is if exists in node\n",
    "    if 'http' in cols[0]:\n",
    "        split = cols[0]\n",
    "    else:\n",
    "        # Strip leading '/' and split by '/'s\n",
    "        split = cols[0].strip('/').split('/')[2:]\n",
    "    if not isinstance(split, list):\n",
    "        split = [split]\n",
    "    name, pos, hypernym = np.nan, np.nan, np.nan\n",
    "    # Extract PoS tag or 'family' word based on how nodes are constructed\n",
    "    if len(split) > 2:\n",
    "        if split[2] in ['wp', 'wn']:\n",
    "            hypernym = split[3]\n",
    "            split = split[:-2]\n",
    "    if len(split) > 1:\n",
    "        pos = split[1]\n",
    "    name = split[0]\n",
    "    return pd.Series([name, pos, hypernym])\n",
    "\n",
    "\n",
    "def process_relation(relation):\n",
    "    # Relation is much easier cause of 2 possibilities\n",
    "    split = relation.strip('/').split('/')\n",
    "    if len(split) > 2:\n",
    "        return split[2]\n",
    "    else:\n",
    "        return split[1]\n",
    "\n",
    "\n",
    "def process_JSON(json_col):\n",
    "    # Extract JSON data from database\n",
    "    # Eval to go from str to dict\n",
    "    json_col = eval(json_col.values[0])\n",
    "    startSurface, endSurface, surfaceText, weight = np.nan, np.nan, np.nan, np.nan\n",
    "    if 'surfaceStart' in json_col:\n",
    "        startSurface = json_col['surfaceStart']\n",
    "    if 'surfaceEnd' in json_col:\n",
    "        endSurface = json_col['surfaceEnd']\n",
    "    if 'surfaceText' in json_col:\n",
    "        surfaceText = json_col['surfaceText']\n",
    "    if 'weight' in json_col:\n",
    "        weight = json_col['weight']\n",
    "\n",
    "    series = [startSurface, endSurface, surfaceText, weight]\n",
    "\n",
    "    return pd.Series(series)\n",
    "\n",
    "\n",
    "def get_language_abbr(language):\n",
    "    if language in lang_dict:\n",
    "        return lang_dict[language]\n",
    "    else:\n",
    "        raise NotImplementedError('Language not implemented or not present')\n",
    "\n",
    "\n",
    "assertion_dir = \"/home/jack/Desktop/NN/clean/datasets/conceptnet/\"\n",
    "conceptnet = ConceptNet(assertion_dir+'assertions.csv', language='english', save_language=True)\n",
    "conceptnet_pkl = ConceptNet(assertion_dir+'assertions_english.pkl')\n",
    "query_result = conceptnet.get_query(start=['start'], end=['end1', 'end2'], relation='relation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antonym(start, end = [], relation =  \"Antonym\"):\n",
    "    start_end = conceptnet.get_query(start=start, end=end, relation=relation)\n",
    "    #print('Raw form of data:')\n",
    "    #print(start_end.get_raw_dataframe().head())\n",
    "    start_end.process_data()\n",
    "    #print(start_end.processed_df.to_string())\n",
    "    #print('You can call query on query, so that just by calling results on the previous answer, you obtain:')\n",
    "    #print('You are now querying the database with only %d entries' % len(start_end))\n",
    "    full_query = start_end.get_query(relation=relation, timing=True)\n",
    "    full_query.process_data()\n",
    "    #print(full_query.processed_df.to_string())\n",
    "    #print(\".. \", full_query.processed_df.groupby(['start','end', 'weight'])['weight'])\n",
    "    max_weight_index =  None\n",
    "    max_weight_index = full_query.processed_df.groupby(['start','end', 'weight'])['weight'].max().argmax()\n",
    "    return full_query.processed_df['endSurface'][max_weight_index]\n",
    "\n",
    "get_antonym(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_antonyms_from_api(search_term, relation = \"/r/Antonym\"):\n",
    "    response = requests.get('http://api.conceptnet.io/query?node=/c/en/<SEARCH_TERM>&rel=<RELATION_TERM>&limit=1000'.replace(\"<SEARCH_TERM>\", search_term).replace(\"<RELATION_TERM>\", relation))\n",
    "    obj = response.json()\n",
    "    return [(x[\"start\"][\"label\"], x[\"weight\"]) for x in obj[\"edges\"]]\n",
    "\n",
    "\n",
    "\n",
    "def select_max_weight_antonym(edge_list):\n",
    "    if len(edge_list)>0:\n",
    "        index_of_max = max([(x[1],index) for index,x in enumerate(edge_list)])[1]\n",
    "        return edge_list[index_of_max][0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_antonym(search_term, fallback = False):\n",
    "    antonym_resp = get_antonyms_from_api(search_term)\n",
    "    antonyms = select_max_weight_antonym(antonym_resp)\n",
    "    if antonyms != None:\n",
    "        return antonyms\n",
    "\n",
    "    # Fallback\n",
    "    if fallback:\n",
    "        derived_resp = get_antonyms_from_api(search_term, \"/r/DerivedFrom\")\n",
    "        derived = select_max_weight_antonym(derived_resp)\n",
    "        if derived != None:\n",
    "            return derived\n",
    "\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resaid\n"
     ]
    }
   ],
   "source": [
    "print(get_antonym(\"said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def read_file(path):\n",
    "    with open(path) as fp:\n",
    "        lines = fp.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def get_attrs(text):\n",
    "    return text.split(\"<CON_START>\")[0].replace(\"<ATTR_WORDS>\",\"\").strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['definitely', 'venue']\n",
      "[['better', 'better'], ['approximately', 'multivenue'], ['untasted', 'overwatered'], ['resaid'], ['ca', 'nonpharmacy'], ['right', 'tookest'], ['wonderful', 'much'], ['approximately', 'glad'], ['owner', 'heardest'], ['enough']]\n",
      "[['large', 'destroy'], ['enjoyingly', 'mediocre'], ['affordable', 'evil'], ['sandwichy', 'preloved'], ['unsigned', 'fell'], ['approximately', 'recommend'], ['highly', 'recommend'], ['otherwise', 'mediocre'], ['evil', 'evil'], ['soot oh', 'popular']]\n"
     ]
    }
   ],
   "source": [
    "''' Load Atts '''\n",
    "data_dir = \"/home/jack/Desktop/NN/clean/datasets/yelp\"\n",
    "\n",
    "ref0_own_Att = read_file(data_dir+\"/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/reference_0.txt\") # Reference data for delete_generate model\n",
    "ref1_own_Att = read_file(data_dir+\"/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/reference_1.txt\") # Reference data for delete_generate model\n",
    "\n",
    "ref_out_path = data_dir + \"/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/\"\n",
    "\n",
    "print(get_attrs(ref0_own_Att[1]))\n",
    "\n",
    "ref0_att_antonyms = []\n",
    "ref1_att_antonyms = []\n",
    "\n",
    "for atts in ref0_own_Att:\n",
    "    att_antonyms = []\n",
    "    for word in get_attrs(atts):\n",
    "        antonym = get_antonym(word, True)\n",
    "        if antonym != None:\n",
    "            att_antonyms.append(antonym)\n",
    "    ref0_att_antonyms.append(att_antonyms)\n",
    "\n",
    "for atts in ref1_own_Att:\n",
    "    att_antonyms = []\n",
    "    for word in get_attrs(atts):\n",
    "        antonym = get_antonym(word, True)\n",
    "        if antonym != None:\n",
    "            att_antonyms.append(antonym)\n",
    "    ref1_att_antonyms.append(att_antonyms)\n",
    "\n",
    "\n",
    "print(ref0_att_antonyms[:10])\n",
    "print(ref1_att_antonyms[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ref_out_path+\"reference_conceptnet_0.txt\", 'w') as outfile:\n",
    "    for index,words in enumerate(ref0_att_antonyms):\n",
    "        if len(words) > 0:\n",
    "            outfile.write(\"<ATTR_WORDS> \"+ \" \".join(words)+ \" <CON_START>\" +ref0_own_Att[index].split(\"<CON_START>\")[1] +\"\\n\")\n",
    "        else: \n",
    "            outfile.write(\"<ATTR_WORDS> \"+ \"<CON_START>\" +ref0_own_Att[index].split(\"<CON_START>\")[1] +\"\\n\")\n",
    "\n",
    "\n",
    "with open(ref_out_path+\"reference_conceptnet_1.txt\", 'w') as outfile:\n",
    "    for index,words in enumerate(ref1_att_antonyms):\n",
    "        if len(words) > 0:\n",
    "            outfile.write(\"<ATTR_WORDS> \"+ \" \".join(words)+ \" <CON_START>\" +ref1_own_Att[index].split(\"<CON_START>\")[1] +\"\\n\")\n",
    "        else: \n",
    "            outfile.write(\"<ATTR_WORDS> \" +\"<CON_START>\" +ref1_own_Att[index].split(\"<CON_START>\")[1] +\"\\n\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cleantransfer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1262644d422a82105623d802af89e6e9ed007275a479308d900e7081e9b63df8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
